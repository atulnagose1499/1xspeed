<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />

    <!-- favicon -->
    <link
      rel="apple-touch-icon"
      sizes="180x180"
      href="./favicon_package_v0.16/apple-touch-icon.png"
    />
    <link
      rel="icon"
      type="image/png"
      sizes="32x32"
      href="/./favicon_package_v0.16/favicon-32x32.png"
    />
    <link
      rel="icon"
      type="image/png"
      sizes="16x16"
      href="./favicon_package_v0.16/favicon-16x16.png"
    />
    <link rel="manifest" href="./favicon_package_v0.16/site.webmanifest" />
    <link
      rel="mask-icon"
      href="./favicon_package_v0.16/safari-pinned-tab.svg"
      color="#5bbad5"
    />
    <meta name="msapplication-TileColor" content="#da532c" />
    <meta name="theme-color" content="#ffffff" />

    <link rel="stylesheet" href="../css/stqa.css" />
    <script src="script.js"></script>
    <title>MCA-Gyan</title>
  </head>
  <body>
    <!-- top banner -->
    <header class="top-banner">
      <div class="container">
        <div class="small-bold-text banner-text">
          üë©üèª‚Äçüíªwhole syllabus study material provided by üß† MCA-Gyan üìö
        </div>
      </div>
    </header>
    <br /><br /><br /><br />
    <section>
      <div class="container">
        <div class="frame1">
          <embed
            class="pdf"
            src="../pdf/STQA-Syllabus.pdf"
            type="application/pdf"
            width="100%"
            height="600px"
          />.
        </div>
      </div>
      <div class="unitNo1">
        <details>
          <summary>1 1. Software Quality Assurance Fundamentals</summary>
          <br />
          <details>
            <br />
            <summary>1 1. Software Quality Assurance Fundamentals</summary>
            <br />
            <p>Software Quality Assurance (SQA) is the process of verifying that a software product meets specified quality standards and is free of defects. It includes a wide range of activities, such as reviewing requirements, designing and executing tests, and monitoring the software development process. The goal of SQA is to ensure that the final product is fit for its intended purpose and meets the needs of the customer. SQA is an integral part of the software development process and is often carried out by a dedicated team or individual, separate from the development team.</p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>
              1.1. Definition of Quality, Quality Assurance, Quality Control,
              Difference between QA and QC, Software Quality Assurance
              Challenges
            </summary>
            <br />
            <p>Quality: The degree to which a product or service meets the customer's expectations and requirements.
              <br /><br />
              Quality Assurance (QA): The process of ensuring that a product or service meets the specified quality standards. It involves planning, monitoring, and evaluating the development process to identify and prevent defects before the product is released.
              <br /><br />
              Quality Control (QC): The process of inspecting and testing a product or service to ensure that it meets the specified quality standards. It involves identifying and correcting defects after the product has been developed.
              <br /><br />
              The difference between QA and QC is that QA focuses on preventing defects from occurring, while QC focuses on identifying and correcting defects after they have occurred. QA is proactive, while QC is reactive.
              <br /><br />
              Software Quality Assurance (SQA) is the process of verifying that a software product meets specified quality standards and is free of defects. SQA includes a wide range of activities, such as reviewing requirements, designing and executing tests, and monitoring the software development process.
              <br /><br />
              Challenges in Software Quality Assurance include:
              <br /><br />
              Ensuring that the software meets the needs of the customer and is fit for its intended purpose.<br /><br />
              Keeping up with rapidly changing technology and industry standards.<br /><br />
              Managing and coordinating the work of multiple teams and individuals involved in the software development process.<br /><br />
              Balancing the need for thorough testing with the need to release the software in a timely manner.<br /><br />
              Managing and reducing the cost of quality assurance activities.<br /><br />
              Maintaining consistency and compatibility across different platforms and devices.<br /><br />
              Keeping up with the increasing complexity of software systems.<br /><br />
              Managing and prioritizing defects and feature requests.<br /><br />
              Ensuring that the software is secure and can protect sensitive data.<br /><br />
              Managing and testing the software in an Agile environment.</p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>
              1.2. Software Quality Assurance, SQA Planning & Standards (ISO
              9000, Six Sigma)
            </summary>
            <br />
            <p>Software Quality Assurance (SQA) planning is the process of identifying and organizing the activities and resources needed to ensure that a software product meets the specified quality standards. It involves creating a detailed plan that outlines the objectives, scope, and schedule of the SQA process, as well as the roles and responsibilities of the individuals and teams involved.
              <br /><br />
              One of the key components of SQA planning is the selection of standards and guidelines to follow. ISO 9000 is a widely recognized set of international standards for quality management systems. It provides a framework for ensuring that products and services meet the needs of customers and other stakeholders, and that the processes used to develop and deliver those products and services are efficient and effective.
              <br /><br />
              Six Sigma is a data-driven approach to quality improvement that aims to eliminate defects in products and services by identifying and removing the causes of variability in processes. Six Sigma is a methodology for quality assurance that uses statistical tools and techniques to reduce the number of defects in a process to a very low level. Six Sigma focuses on continuous improvement, reducing defects, and increasing customer satisfaction.
              <br /><br />
              Both ISO 9000 and Six Sigma can be used to help organizations improve their software development processes and ensure that their products meet the needs of their customers. However, ISO 9000 focuses more on the processes and documentation, while Six Sigma focuses more on the data-driven, statistical approach to quality improvement. Organizations can choose to adopt one or both of these standards to improve their software quality assurance processes.</p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>1.3. SQA Activities</summary>
            <br />
            <p>Software Quality Assurance (SQA) activities are the specific tasks and procedures that are carried out to ensure that a software product meets the specified quality standards. These activities can be broadly categorized into the following:
              <br /><br />
              Planning: This includes defining the objectives and scope of the SQA process, creating a detailed plan for the SQA activities, and identifying the roles and responsibilities of the individuals and teams involved.
              <br /><br />
              Requirements Analysis: This includes reviewing and analyzing the software requirements to ensure that they are clear, complete, and consistent, and that they meet the needs of the customer.
              <br /><br />
              Design and Implementation: This includes designing and executing tests to ensure that the software meets the requirements and is free of defects. This can include both manual and automated testing.
              <br /><br />
              Inspection and Reviews: This includes reviewing and inspecting the software design and code to identify defects and ensure that it meets the specified quality standards.
              <br /><br />
              Testing: This includes executing a wide range of tests to ensure that the software is free of defects and meets the requirements. This can include unit testing, integration testing, system testing, and acceptance testing.
              <br /><br />
              Configuration Management: This includes managing and controlling the changes to the software throughout the development process to ensure that the correct version of the software is being tested and deployed.
              <br /><br />
              Auditing and Reporting: This includes conducting audits of the software development process to ensure that it is in compliance with the specified quality standards, and reporting any issues or defects that are found.
              <br /><br />
              Maintenance: This includes monitoring the software after it has been released to ensure that it continues to meet the needs of the customer and is free of defects.
              <br /><br />
              Overall, SQA activities are an integral part of the software development process and are designed to ensure that the final product meets the needs of the customer and is fit for its intended purpose.</p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>1.4. Building Blocks of SQA</summary>
            <br />
            <p>The building blocks of Software Quality Assurance (SQA) are the fundamental elements that are required to establish an effective SQA process. These building blocks include:
              <br /><br />
              Quality Policy: A statement of the organization's commitment to quality and the standards and guidelines that will be used to achieve it.
              <br /><br />
              Quality Planning: The process of identifying and organizing the resources and activities needed to ensure that a software product meets the specified quality standards.
              <br /><br />
              Standards and Procedures: The standards and procedures that will be used to guide the SQA process, such as ISO 9000 or Six Sigma.
              <br /><br />
              Reviews and Audits: The process of inspecting and evaluating the software development process to ensure that it is in compliance with the specified quality standards.
              <br /><br />
              Configuration Management: The process of managing and controlling the changes to the software throughout the development process to ensure that the correct version of the software is being tested and deployed.
              <br /><br />
              Testing: The process of executing a wide range of tests to ensure that the software is free of defects and meets the requirements.
              <br /><br />
              Defect Management: The process of identifying, tracking, and resolving defects in the software.
              <br /><br />
              Training and Education: The process of educating and training the individuals and teams involved in the software development process to ensure that they have the knowledge and skills needed to create high-quality software.
              <br /><br />
              Management Review: The process of reviewing the overall performance of the SQA process and identifying opportunities for improvement.
              <br /><br />
              Overall, these building blocks provide the foundation for an effective SQA process, and are necessary for ensuring that the final software product meets the needs of the customer and is fit for its intended purpose.
              
              
              
              </p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>1.5. Software Quality factors</summary>
            <br />
            <p>Software Quality factors are the characteristics of a software product that determine its ability to meet the needs of the customer and be fit for its intended purpose. Some common software quality factors include:
              <br /><br />
              Functionality: The ability of the software to perform the functions that it is designed to perform, and to meet the needs of the customer.
              <br /><br />
              Reliability: The ability of the software to perform its functions correctly and consistently over time, and to be dependable and free from defects.
              <br /><br />
              Usability: The ease of use and user-friendliness of the software, and its ability to be understood and used by the intended user.
              <br /><br />
              Efficiency: The speed and performance of the software, and its ability to use system resources effectively.
              <br /><br />
              Maintainability: The ability of the software to be modified, updated, and maintained over time, and the ability to keep the product running and in good condition.
              <br /><br />
              Portability: The ability of the software to be easily transferred and installed on different hardware and software platforms.
              <br /><br />
              Compatibility: The ability of the software to work with other software and systems.
              <br /><br />
              Security: The ability of the software to protect against unauthorized access, use, disclosure, disruption, modification, or destruction.
              <br /><br />
              Compliance: The ability of the software to meet the industry standards, regulations, and legal requirements
              <br /><br />
              Scalability: The ability of the software to handle increased workloads and to adapt to changing user needs.
              <br /><br />
              These factors are important to consider when developing a software product as they can greatly impact the user experience and the overall success of the product.</p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>
              1.6. Software Quality Metrics: Process Metrics & Product Metrics
            </summary>
            <br />
            <p>Software Quality Metrics are measurements used to evaluate the quality of a software product or process. There are two main categories of software quality metrics: process metrics and product metrics.
              <br /><br />
              Process Metrics: These metrics measure the effectiveness and efficiency of the software development process. They can be used to identify areas where the process can be improved and to track progress over time. Some examples of process metrics include:
              <br /><br />
              Defect density: The number of defects per unit of code<br /><br />
              Code coverage: The percentage of code that has been tested<br /><br />
              Lead time: The time it takes for a feature to be developed and delivered<br /><br />
              Cycle time: The time it takes for a feature to go through the entire development process<br /><br />
              Product Metrics: These metrics measure the quality of the final software product. They can be used to identify defects and areas where the product can be improved. Some examples of product metrics include:
              <br /><br />
              Reliability: The ability of the software to perform its functions correctly and consistently over time<br /><br />
              Usability: The ease of use and user-friendliness of the software<br /><br />
              Performance: The speed and responsiveness of the software<br /><br />
              Security: The ability of the software to protect against unauthorized access, use, disclosure, disruption, modification, or destruction<br /><br />
              Scalability: The ability of the software to handle increased workloads and to adapt to changing user needs<br /><br />
              It's important to note that, different types of software have different quality requirements, so the metrics that are used will vary depending on the type of software and the needs of the customer. Also, it's crucial to have a plan to collect, store, and analyze data to measure software quality metrics, otherwise, the metrics are meaningless.</p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>
              1.7. Software Reliability & Reliability Measurement Factors:
              ROCOF, MTTF, MTTR, MTBF, POFOD, Availability
            </summary>
            <br />
            <p>Software reliability refers to the ability of a software system to perform its intended functions without failure for a specified period of time. Reliability measurement factors are used to quantify and evaluate the reliability of a software system. Some common reliability measurement factors include:
              <br /><br />
              ROCOF (Reliability of Component): The probability that a component will operate correctly over a given period of time.
              <br /><br />
              MTTF (Mean Time to Failure): The average time between failures for a software component or system.
              <br /><br />
              MTTR (Mean Time to Repair): The average time it takes to repair a failed component or system.
              <br /><br />
              MTBF (Mean Time Between Failures): The average time between failures for a software component or system, calculated as MTTF + MTTR.
              <br /><br />
              POFOD (Probability of Failure On Demand): The probability that a software component or system will fail when called upon to perform its intended function.
              <br /><br />
              Availability: The proportion of time that a software component or system is in a functioning state, calculated as MTBF / (MTBF + MTTR).
              <br /><br />
              It's important to note that, these factors are not only used for software but also for other systems, and the definition and formula for each factor can vary by industry or application. Also, collecting data for these factors is crucial in order to measure and evaluate reliability, and should be part of the software development process.</p>
            <br /><br /><br />
          </details>
          <br />
        </details>
      </div>
      <br />
      <div class="unitNo2">
        <details>
          <summary>2 2. Software Testing Fundamentals</summary>
          <br />
          <details>
            <br />
            <summary>2 2. Software Testing Fundamentals</summary>
            <br />
            <p>Software testing is the process of evaluating a software system or its component(s) with the intent to find whether it satisfies the specified requirements or not, and to identify any defects. It's a critical step in the software development process that ensures the quality and reliability of the final product.
              <br /><br />
              The main goals of software testing are to identify defects, to measure the quality of the product, and to provide information about the product's capabilities and limitations. There are several types of software testing, including:
              <br /><br />
              Unit testing: Testing individual units or components of the software to ensure they function correctly.
              <br /><br />
              Integration testing: Testing how different units or components of the software interact with each other to ensure they work together correctly.
              <br /><br />
              Functional testing: Testing the software's functionality to ensure it meets the specified requirements.
              <br /><br />
              System testing: Testing the entire software system to ensure it works as intended in a real-world environment.
              <br /><br />
              Acceptance testing: Testing the software to ensure it meets the needs and expectations of the customer or end-user.
              <br /><br />
              Performance testing: Testing the software's performance to ensure it can handle the expected workload and user traffic.
              <br /><br />
              Security testing: Testing the software's security to identify vulnerabilities and ensure it can protect against unauthorized access or attacks.
              <br /><br />
              It's important to note that software testing is an ongoing process that should be performed throughout the software development life cycle, and should include both manual and automated testing methods.</p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>2.1. Definition & Objectives of Testing</summary>
            <br />
            <p>Testing is the process of evaluating a system or its component(s) using a set of predefined procedures, with the intent to find whether it satisfies the specified requirements or not, and to identify any defects.
              <br /><br />
              The main objectives of testing are:
              <br /><br />
              To find defects: Testing is conducted to identify defects or bugs in the software system.
              <br /><br />
              To measure the quality of the product: Testing helps to measure the quality of the software product and to identify areas where improvements can be made.
              <br /><br />
              To provide information about the product's capabilities and limitations: Testing provides information about the product's capabilities and limitations, which can be used to make informed decisions about its use and deployment.
              <br /><br />
              To ensure the software meets the customer's needs: Testing helps to ensure that the software meets the needs and expectations of the customer or end-user.
              <br /><br />
              To reduce the risk of software failure: By identifying and fixing defects early in the development process, testing helps to reduce the risk of software failure in the production environment.
              <br /><br />
              To support the maintenance and evolution of the software system: Testing helps to support the maintenance and evolution of the software system by providing information about the system's structure and behavior.
              <br /><br />
              Testing is an integral part of the software development life cycle and is performed throughout the process, from the early stages of development to the final stages of deployment and maintenance.
              
              
              
              </p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>2.2. Role of testing and its effect on quality</summary>
            <br />
            <p>Testing plays a critical role in ensuring the quality of a software system. Through testing, defects and issues can be identified and corrected early in the development process, before the software is released to the customer. This helps to ensure that the final product meets the specified requirements and is free of defects.
              <br /><br />
              Testing also helps to measure the quality of the software product and to identify areas where improvements can be made. This can be done by establishing quality metrics and using them to evaluate the software at different stages of the development process.
              <br /><br />
              By thoroughly testing a software system, the tester can gain a deep understanding of the system's capabilities and limitations, which can be used to make informed decisions about its use and deployment. This is especially important in the case of critical systems, such as those used in medical or aerospace applications, where a failure could have serious consequences.
              <br /><br />
              Testing also helps to reduce the risk of software failure in the production environment. By identifying and fixing defects early in the development process, testing can prevent costly and time-consuming issues from arising later on.
              <br /><br />
              In short, testing plays a crucial role in ensuring the quality of a software system. It helps to identify defects, measure the quality of the product, provide information about the product's capabilities and limitations, ensure that the software meets the customer's needs, reduce the risk of software failure, and support the maintenance and evolution of the software system.
              
              
              
              </p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>
              2.3. Causes of software failure: Definition of -Error, Bug, Fault,
              Defect and Failure,
            </summary>
            <br />
            <p>In software development, several terms are used to describe issues that can lead to software failure. These terms include:
              <br /><br />
              Error: An error is a human action or inaction that results in an incorrect or unexpected outcome. Examples of errors include a programmer accidentally deleting a line of code or a tester providing incorrect input to the system.
              <br /><br />
              Bug: A bug is a software defect or problem that causes the system to behave in an unintended or unexpected manner. This can include issues such as system crashes, data loss, or incorrect calculations.
              <br /><br />
              Fault: A fault is a defect or issue in the software that can cause a failure. A fault can be caused by an error, such as a programmer introducing a bug, or by a problem with the software requirements or design.
              <br /><br />
              Defect: A defect is a non-conformance to the requirement or any missing feature in the software product.
              <br /><br />
              Failure: A failure is the inability of a system or component to perform its intended function. This can occur due to errors, bugs, faults, or defects in the software.
              <br /><br />
              It is important to note that these terms are often used interchangeably, and the specific meaning may depend on the context in which they are used. However, in general, an error is a human action or inaction that leads to an incorrect outcome, a bug is a software defect that causes the system to behave in an unintended manner, a fault is a defect or issue in the software that can cause a failure, a defect is a non-conformance to the requirement or any missing feature in the software product, and a failure is the inability of a system or component to perform its intended function.</p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>2.4. Economics of Testing</summary>
            <br />
            <p>The economics of testing refers to the cost-benefit analysis of various testing strategies and techniques. The goal of software testing is to identify and correct defects as early and as cost-effectively as possible. In order to achieve this goal, it is important to consider the costs and benefits of different testing activities and to make informed decisions about where to allocate resources.
              <br /><br />
              The costs associated with testing include not just the direct costs of performing the tests, but also the indirect costs such as the cost of defects that are not identified and corrected during testing. The benefits of testing include improved software quality, increased customer satisfaction, and reduced costs associated with post-release defects.
              <br /><br />
              The economics of testing also involves trade-offs between the cost of testing and the risk of defects. For example, more thorough testing may cost more but may also result in fewer defects being released to the customer. On the other hand, less thorough testing may cost less but may also result in more defects being released to the customer.
              <br /><br />
              In order to make informed decisions about testing, it is important to consider the costs and benefits of different testing strategies and to use metrics to measure the effectiveness of testing activities.
              <br /><br />
              In summary, the economics of testing is a cost-benefit analysis of different testing strategies and techniques, taking into account the costs and benefits of testing, the trade-off between cost and risk, and the use of metrics to measure the effectiveness of testing.
              
              
              
              </p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>2.5. Seven Testing Principles</summary>
            <br />
            <p>There are several principles that guide the practice of software testing. Some of the most commonly referenced principles include:
              <br /><br />
              Testing is context-dependent: Testing should be tailored to the specific context in which the software will be used, taking into account factors such as the system's intended use, the technology used, and the target users.
              <br /><br />
              Early testing: Testing should begin as early as possible in the software development process to identify and correct defects as early as possible.
              <br /><br />
              Defect clustering: A small number of defects are likely to be responsible for the majority of failures, so testing should focus on identifying these critical defects.
              <br /><br />
              Pesticide paradox: Over time, testing against a fixed set of test cases will become less effective, so it is necessary to regularly update the test cases.
              <br /><br />
              Testing is an investigation: Testing is an active process of investigating the product to identify defects.
              <br /><br />
              Absence of error fallacy: Testing can only demonstrate the presence of defects, not their absence.
              <br /><br />
              Exhaustive testing is impossible: It is not possible to test all possible inputs and preconditions, so testing should focus on high-risk areas and use risk-based strategies to prioritize testing efforts.
              <br /><br />
              These principles are fundamental to the testing process and should be considered when planning and executing a testing strategy.</p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>2.6. Software Testing Life cycle</summary>
            <br />
            <p>The Software Testing Life Cycle (STLC) is a series of activities that are performed in a systematic and controlled manner to ensure that the software is of high quality and meets the requirements of the customer. The STLC consists of several phases that are followed in a specific order. The main phases of the STLC are:
              <br /><br />
              Requirements Analysis: In this phase, the software requirements are studied and analyzed. This helps in understanding the scope of testing and identifying the areas that need to be tested.
              <br /><br />
              Test Planning: In this phase, a test plan is created that outlines the overall testing strategy and approach. This includes identifying the testing objectives, the resources required, and the schedule for testing.
              <br /><br />
              Test Design: In this phase, the test cases are designed. This includes identifying the test inputs, expected outputs, and the conditions under which the tests will be executed.
              <br /><br />
              Test Execution: In this phase, the test cases are executed. This includes running the tests, collecting the test results, and comparing them with the expected results.
              <br /><br />
              Test Closure: In this phase, the testing process is closed. This includes documenting the test results, analyzing the defects found, and taking corrective actions to fix the defects.
              <br /><br />
              Test Maintenance: In this phase, the test cases, test data and test environment are maintained and updated.
              <br /><br />
              The STLC is an iterative process that is repeated as necessary until the software is of high quality and meets the requirements of the customer. The STLC helps in ensuring that the software is tested thoroughly, defects are identified and corrected early, and the software is of high quality.
              
              
              
              </p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>
              2.7. Validation & Verification Concepts - V Model and W Model
            </summary>
            <br />
            <p>Validation and verification are two important concepts in software development and testing.
              <br /><br />
              Validation is the process of evaluating a system or component during or at the end of the development process to determine whether it satisfies the specified requirements. It is done to ensure that the software is fit for its intended use.
              <br /><br />
              Verification, on the other hand, is the process of evaluating a system or component during the development process to determine whether the products of a given phase of the life cycle meet the specified requirements for that phase. It is done to ensure that the software is built as per the requirements.
              <br /><br />
              The V-Model and the W-Model are two popular models that depict the relationships between validation and verification activities and the different phases of the software development life cycle.
              <br /><br />
              The V-Model is a linear representation of the software development life cycle where the vertical axis represents the life cycle phases, and the horizontal axis represents the corresponding validation and verification activities. The V-Model is used to depict the relationship between the development and testing phases of the software development life cycle.
              <br /><br />
              The W-Model is an extension of the V-Model. It is an iterative representation of the software development life cycle where each iteration goes through the entire life cycle phases, including validation and verification activities. The W-Model is used to depict the relationship between the development, testing and maintenance phases of the software development life cycle.
              <br /><br />
              In summary, V-Model and W-Model are two popular models used to depict the relationships between validation and verification activities and the different phases of the software development life cycle. The V-Model is used for linear representation of the software development life cycle, while W-Model is used for iterative representation of the software development life cycle.</p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>
              2.8. Agile Testing- Test Driven Software Development
            </summary>
            <br />
            <p>Agile testing is a method of software testing that aligns with the Agile software development methodology. It emphasizes on a collaborative, incremental, and iterative approach to testing, where testing is integrated into the development process from the beginning.
              <br /><br />
              Test-Driven Development (TDD) is a technique of Agile testing where the developers write automated tests before writing any code. The tests are written to define the requirements of the software and to ensure that the code meets those requirements. This helps to catch defects early in the development process and to ensure that the software is of high quality.
              <br /><br />
              The process of TDD can be summarized in the following steps:
              <br /><br />
              Write a test case: Write a test case that defines the requirements of the software.<br /><br />
              Run the test: Run the test, which will fail because the code has not been written yet.<br /><br />
              Write the code: Write the code to make the test pass.<br /><br />
              Run the test again: Run the test again, which should pass this time.<br /><br />
              Refactor the code: Refactor the code, if needed, to improve its design or maintainability.<br /><br />
              Repeat the process: Repeat the process for the next requirement.<br /><br />
              TDD helps to improve the quality of the software, reduces defects, and ensures that the software meets the requirements. It also helps to improve communication between the developers and the customers, as the requirements are clearly defined in the form of test cases.</p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>2.9. Levels of Testing-</summary>
            <br />
            <p>There are several levels of testing that are typically performed during the software development life cycle to ensure the quality of the software. These levels of testing include:
              <br /><br />
              Unit testing: Unit testing is the process of testing individual units of code, such as functions or methods, to ensure that they are working correctly. This is typically done by the developer who wrote the code.
              <br /><br />
              Integration testing: Integration testing is the process of testing how different units of code work together. This is done to ensure that there are no issues when different modules are combined.
              <br /><br />
              System testing: System testing is the process of testing the entire software system as a whole. This is done to ensure that the software meets the requirements and that it is fit for its intended use.
              <br /><br />
              Acceptance testing: Acceptance testing is the process of testing the software to ensure that it meets the acceptance criteria defined by the customer. This is done to ensure that the customer is satisfied with the software and that it meets their needs.
              <br /><br />
              Performance testing: Performance testing is the process of testing the software to ensure that it can handle the expected load and usage. This is done to ensure that the software can perform well under different conditions and that it can handle the expected number of users.
              <br /><br />
              Security testing: Security testing is the process of testing the software to ensure that it is secure and that it can protect against unauthorized access or attacks.
              <br /><br />
              Usability testing: Usability testing is the process of testing the software to ensure that it is easy to use and that it meets the needs of the users.
              <br /><br />
              All these testing levels are done to ensure the quality of the software and to identify any issues early on in the development process, so that they can be fixed before the software is released to the customer.</p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>2.9.1. Unit (Component) Testing</summary>
            <br />
            <p>Unit testing is a method of testing individual units or components of software to ensure that they are working correctly. It is typically done during the development process and is used to catch bugs and errors early on before they become more complex and difficult to fix.
              <br /><br />
              Unit tests are usually written by developers and are designed to test specific functionality of the software. They are typically automated and can be run multiple times, making it easy to catch any issues that may arise during the development process.
              <br /><br />
              Unit tests are typically focused on testing individual functions or methods, rather than the overall behavior of the software. This allows developers to isolate and test specific parts of the code, which can make it easier to identify and fix bugs.
              <br /><br />
              Unit tests can be run using various testing frameworks, such as JUnit for Java, NUnit for .NET, or PyUnit for Python. These frameworks provide a set of tools and functions that make it easy to write and run unit tests.
              <br /><br />
              Overall, unit testing helps developers to ensure that their code is working as expected, by verifying that individual components are functioning correctly and thus they can be more confident in the overall functionality of their software.</p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>2.9.2. Integration Testing</summary>
            <br />
            <p>Integration testing is a method of testing how different units or components of software work together. It is typically done after unit testing, and is used to ensure that all the different parts of the software are working together as intended.
              <br /><br />
              Integration tests are designed to test the interactions between different components of the software, and how they work together to perform specific tasks. They are typically automated and can be run multiple times, making it easy to catch any issues that may arise during the development process.
              <br /><br />
              Unlike unit tests, which focus on testing individual functions or methods, integration tests focus on testing the overall behavior of the software. This allows developers to ensure that the software is working as intended, even when different components are combined and interacting with each other.
              <br /><br />
              Integration tests can be run using various testing frameworks, such as JUnit for Java, NUnit for .NET, or PyUnit for Python. These frameworks provide a set of tools and functions that make it easy to write and run integration tests.
              <br /><br />
              Overall, integration testing helps developers to ensure that the different parts of their software are working together as intended, by verifying that the interactions between different components are functioning correctly, and thus they can be more confident in the overall functionality of their software.
              
              
              
              </p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>2.9.3. System Testing</summary>
            <br />
            <p>System testing is a method of testing the overall functionality and performance of a software system. It is typically done after integration testing and is used to ensure that the software meets the requirements and specifications set out in the design phase.
              <br /><br />
              System tests are designed to test the software as a whole, and are focused on testing the system's ability to perform its intended function. They are typically automated and can be run multiple times, making it easy to catch any issues that may arise during the development process.
              <br /><br />
              System tests are usually done by a dedicated testing team and are not written by the developers themselves. They are designed to test the software in a way that simulates how it will be used in the real world, and may include tests such as performance testing, load testing, and stress testing.
              <br /><br />
              System tests can be run using various testing frameworks, such as JUnit for Java, NUnit for .NET, or PyUnit for Python. These frameworks provide a set of tools and functions that make it easy to write and run system tests.
              <br /><br />
              Overall, system testing helps developers to ensure that their software is meeting the needs of the users and that it is working as intended, by verifying that the software is meeting the requirements and specifications set out in the design phase and thus they can be more confident in the overall functionality of their software.</p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>2.9.4. User Acceptance Testing (UAT)</summary>
            <br />
            <p>User Acceptance Testing (UAT) is a method of testing a software system by having actual users of the system test it for functionality and usability. It is typically done near the end of the development process, and is used to ensure that the software meets the needs of the users and that it is usable and easy to use.
              <br /><br />
              UAT is often performed by a group of users who are representative of the target audience for the software. These users will test the software by performing tasks and scenarios that are representative of how the software will be used in the real world. They will also provide feedback on the software's usability and whether it meets their needs.
              <br /><br />
              UAT is an important step in the software development process because it allows the developers to test the software in the context of how it will be used in the real world. It also allows the users to provide feedback on the software, which can be used to improve its functionality and usability.
              <br /><br />
              UAT can be done in various ways, such as beta testing, where a small group of users test the software before it is released, or alpha testing, where the software is tested by internal stakeholders.
              <br /><br />
              Overall, User Acceptance Testing (UAT) is a critical step in the software development process as it helps developers to ensure that the software meets the needs of the users and that it is usable and easy to use, by validating the software's functionality and usability with the users, who will be using it in the real world.</p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>2.10. Test Types</summary>
            <br />
            <p>There are various types of testing that can be used to ensure that software is functioning correctly. Some of the most common types include:
              <br /><br />
              Unit Testing: Testing individual units or components of software to ensure that they are working correctly.
              <br /><br />
              Integration Testing: Testing how different units or components of software work together to ensure that they are functioning correctly when combined.
              <br /><br />
              System Testing: Testing the overall functionality and performance of a software system to ensure that it meets the requirements and specifications set out in the design phase.
              <br /><br />
              User Acceptance Testing (UAT): Testing a software system by having actual users of the system test it for functionality and usability.
              <br /><br />
              Functional Testing: Testing the functionality of a software system to ensure that it meets the requirements and specifications set out in the design phase.
              <br /><br />
              Non-Functional Testing: Testing the non-functional aspects of a software system, such as performance, security, and usability.
              <br /><br />
              Regression Testing: Testing a software system to ensure that changes or updates have not introduced any new bugs or errors.
              <br /><br />
              Performance Testing: Testing a software system to ensure that it can handle the expected load and usage.
              <br /><br />
              Security Testing: Testing a software system to ensure that it is secure and that there are no vulnerabilities that could be exploited.
              <br /><br />
              Usability Testing: Testing a software system to ensure that it is easy to use and understand for the users.
              <br /><br />
              These are some of the most common types of testing, however, there are other types of testing that can be used depending on the specific needs of the software.</p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>2.10.1. Functional testing (Black-box)</summary>
            <br />
            <p>Functional testing, also known as black-box testing, is a method of testing the functionality of a software system to ensure that it meets the requirements and specifications set out in the design phase. This type of testing is focused on the inputs and outputs of the system, rather than its internal structure or implementation.
              <br /><br />
              During functional testing, the tester creates test cases based on the software's requirements and specifications. These test cases are designed to test the different functions and features of the software, such as buttons, menus, and other user interface elements. The tester then executes these test cases and verifies that the software behaves as expected.
              <br /><br />
              Functional testing can be done manually or using automated testing tools. Automated testing tools can be used to create and execute test cases, as well as to store and compare results. This makes it easier to identify any issues or bugs that may be present in the software.
              <br /><br />
              Black-box testing is important because it allows developers to ensure that the software is meeting the needs of the users and that it is working as intended. It also allows developers to identify and fix any issues or bugs that may be present in the software before it is released to users.
              <br /><br />
              Overall, functional testing (black-box) is an important step in the software development process as it helps developers to ensure that the software is meeting the requirements and specifications set out in the design phase and that it is working as intended, by validating the software's functionality.
              
              
              
              </p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>
              2.10.2. Non-functional testing (Testing of software product
              characteristics)
            </summary>
            <br />
            <p>Non-functional testing is a method of testing the non-functional aspects of a software system, such as performance, security, and usability. This type of testing is focused on how well the software system meets the requirements for these non-functional characteristics.
              <br /><br />
              During non-functional testing, the tester creates test cases based on the software's requirements and specifications for non-functional characteristics. These test cases are designed to test the software's performance, security, usability, and other non-functional characteristics. The tester then executes these test cases and verifies that the software meets the requirements for these characteristics.
              <br /><br />
              Performance testing is a type of non-functional testing that is used to evaluate how well the software can handle the expected load and usage. During performance testing, the tester creates test cases that simulate different levels of usage and load, and then evaluates the software's response.
              <br /><br />
              Security testing is another type of non-functional testing that is used to ensure that the software is secure and that there are no vulnerabilities that could be exploited. During security testing, the tester creates test cases to identify potential vulnerabilities in the software and evaluate the software's response to these test cases.
              <br /><br />
              Usability testing is another type of non-functional testing that is used to ensure that the software is easy to use and understand for the users. During usability testing, the tester creates test cases to evaluate the software's usability and user experience, and then evaluates the software's response to these test cases.
              <br /><br />
              Non-functional testing is important because it allows developers to ensure that the software is meeting the requirements for non-functional characteristics, such as performance, security, and usability, and that it is working as intended. It also allows developers to identify and fix any issues or bugs that may be present in the software before it is released to users.
              <br /><br />
              Overall, non-functional testing (testing of software product characteristics) is an important step in the software development process as it helps developers to ensure that the software is meeting the requirements for non-functional characteristics, such as performance, security, and usability, and that it is working as intended, by validating the software's performance, security and usability.</p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>2.10.3. Structural testing (White-box)</summary>
            <br />
            <p>Structural testing, also known as white-box testing, is a method of testing the internal structure and implementation of a software system. This type of testing is focused on the code and internal logic of the system, rather than its inputs and outputs.
              <br /><br />
              During structural testing, the tester creates test cases based on the software's internal structure and code. These test cases are designed to test the different components and functions of the software, such as loops, conditions, and branches. The tester then executes these test cases and verifies that the software behaves as expected.
              <br /><br />
              Structural testing can be done manually or using automated testing tools. Automated testing tools can be used to create and execute test cases, as well as to store and compare results. This makes it easier to identify any issues or bugs that may be present in the software.
              <br /><br />
              White-box testing is important because it allows developers to ensure that the software is working as intended and that it is following best practices for coding and software design. It also allows developers to identify and fix any issues or bugs that may be present in the software before it is released to users.
              <br /><br />
              Overall, structural testing (white-box) is an important step in the software development process as it helps developers to ensure that the software is following best practices for coding and software design and that it is working as intended, by validating the software's internal structure and implementation.
              
              
              
              </p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>
              2.10.4. Testing related to changes - Confirmation (Re-testing) and
              Regression Testing
            </summary>
            <br />
            <p>Confirmation (re-testing) and regression testing are two types of testing that are related to changes in a software system. They are used to ensure that the changes made to the software do not introduce new bugs or issues and that the software continues to work as intended.
              <br /><br />
              Confirmation (re-testing) is a type of testing that is done to confirm that a bug or issue has been fixed. When a bug or issue is reported, it is fixed by the development team and then re-tested to confirm that the fix has resolved the problem. This type of testing is also known as verification testing.
              <br /><br />
              Regression testing is a type of testing that is done to ensure that changes made to the software do not introduce new bugs or issues and that the software continues to work as intended. This type of testing is done after changes have been made to the software to ensure that the changes have not introduced any new issues or affected the functionality of the software.
              <br /><br />
              Regression testing is important because it allows developers to ensure that changes made to the software do not introduce new bugs or issues and that the software continues to work as intended. It also helps developers to identify any issues that may be present in the software before it is released to users.
              <br /><br />
              Overall, confirmation (re-testing) and regression testing are important steps in the software development process as they help developers to ensure that the changes made to the software do not introduce new bugs or issues and that the software continues to work as intended.</p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>2.11. Non-Functional Testing Types</summary>
            <br />
            <p>Non-functional testing is a type of testing that is focused on testing the software's characteristics, rather than its inputs and outputs. Non-functional testing includes several different types of testing that are used to evaluate the software's performance, usability, security, and other characteristics. Some of the most common types of non-functional testing are:
              <br /><br />
              Performance testing: This type of testing is used to evaluate the software's performance, such as its speed, responsiveness, and scalability.
              <br /><br />
              Usability testing: This type of testing is used to evaluate the software's usability, such as its ease of use, user interface, and user experience.
              <br /><br />
              Security testing: This type of testing is used to evaluate the software's security, such as its vulnerability to hacking, data breaches, and other security threats.
              <br /><br />
              Stress testing: This type of testing is used to evaluate the software's ability to handle heavy loads, such as high traffic or large amounts of data.
              <br /><br />
              Compatibility testing: This type of testing is used to evaluate the software's compatibility with different operating systems, browsers, and devices.
              <br /><br />
              Localization testing: This type of testing is used to evaluate the software's ability to support different languages and regions.
              <br /><br />
              Accessibility testing: This type of testing is used to evaluate the software's ability to support users with disabilities and to comply with accessibility standards.
              <br /><br />
              Overall, non-functional testing is an important step in the software development process as it helps developers to evaluate the software's characteristics and ensure that it meets the needs of users.</p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>2.11.1. Performance (Load & Stress)</summary>
            <br />
            <p>Performance testing, also known as load and stress testing, is a type of non-functional testing that is used to evaluate the software's performance under different loads and conditions. This type of testing is used to measure the software's speed, responsiveness, and scalability.
              <br /><br />
              Load testing is a type of performance testing that is used to evaluate the software's performance under normal loads. During load testing, the software is tested with a normal number of users or transactions to determine how it performs under normal conditions.
              <br /><br />
              Stress testing is a type of performance testing that is used to evaluate the software's performance under extreme loads. During stress testing, the software is tested with an excessive number of users or transactions to determine how it performs under extreme conditions.
              <br /><br />
              Performance testing is important because it allows developers to identify and fix any issues or bugs that may be present in the software before it is released to users. It also helps developers to ensure that the software is able to handle heavy loads, such as high traffic or large amounts of data.
              <br /><br />
              Overall, performance testing (load & stress) is an important step in the software development process as it helps developers to evaluate the software's performance under different loads and conditions, identify and fix any issues or bugs, and ensure that the software is able to handle heavy loads.</p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>2.11.2. Usability</summary>
            <br />
            <p>Usability testing is a type of non-functional testing that is used to evaluate the software's usability, or how easy it is for users to interact with and use the software. This type of testing is focused on the user interface and user experience.
              <br /><br />
              During usability testing, users are asked to perform specific tasks using the software while their actions and interactions with the software are observed and recorded. The observations and recordings are then analyzed to identify any issues or problems with the software's usability.
              <br /><br />
              Usability testing is important because it allows developers to identify any issues or problems with the software's user interface and user experience before it is released to users. It also helps developers to ensure that the software is easy for users to interact with and use, which can improve user satisfaction and increase the likelihood of the software being adopted and used.
              <br /><br />
              Overall, usability testing is an important step in the software development process as it helps developers to evaluate the software's usability and identify any issues or problems with the user interface and user experience before it is released to users. It helps to ensure that the software is easy for users to interact with and use, which can improve user satisfaction and increase the likelihood of the software being adopted and used.
              
              
              
              </p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>2.11.3. Maintainability</summary>
            <br />
            <p>Maintainability testing is a type of non-functional testing that is used to evaluate the software's ability to be maintained, or how easy it is to modify or update the software. This type of testing is focused on the software's design, code, and documentation.
              <br /><br />
              During maintainability testing, the software's design, code, and documentation are analyzed to identify any issues or problems that may make it difficult to modify or update the software. The test team may also conduct testing to ensure that the software can be easily modified or updated without causing issues or breaking any functionality.
              <br /><br />
              Maintainability testing is important because it allows developers to identify any issues or problems that may make it difficult to modify or update the software before it is released to users. It also helps developers to ensure that the software is designed in a way that makes it easy to modify or update, which can reduce the cost and time required to maintain the software.
              <br /><br />
              Overall, maintainability testing is an important step in the software development process as it helps developers to evaluate the software's ability to be maintained, identify any issues or problems that may make it difficult to modify or update the software, and ensure that the software is designed in a way that makes it easy to modify or update. This can reduce the cost and time required to maintain the software, which can improve the software's overall value.</p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>2.11.4. Portability</summary>
            <br />
            <p>Portability testing is a type of non-functional testing that is used to evaluate the software's ability to be ported, or moved, to different environments or platforms. This type of testing is focused on the software's compatibility with different hardware, operating systems, and software.
              <br /><br />
              During portability testing, the software is tested on different environments or platforms to identify any issues or problems that may arise when the software is moved to a new environment or platform. The test team may also conduct testing to ensure that the software can be easily moved or ported to different environments or platforms without causing issues or breaking any functionality.
              <br /><br />
              Portability testing is important because it allows developers to identify any issues or problems that may arise when the software is moved to a new environment or platform before it is released to users. It also helps developers to ensure that the software is designed in a way that makes it easy to be moved or ported to different environments or platforms, which can increase the software's overall value and usability.
              <br /><br />
              Overall, portability testing is an important step in the software development process as it helps developers to evaluate the software's ability to be ported, identify any issues or problems that may arise when the software is moved to a new environment or platform, and ensure that the software is designed in a way that makes it easy to be moved or ported to different environments or platforms. This can increase the software's overall value and usability and make it more accessible to a wider range of users.</p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>2.11.5. Security</summary>
            <br />
            <p>Security testing is a type of non-functional testing that is used to evaluate the software's ability to protect against unauthorized access, use, disclosure, disruption, modification, or destruction of information. This type of testing is focused on identifying vulnerabilities, threats, and weaknesses in the software's security.
              <br /><br />
              During security testing, various techniques and tools are used to identify and assess the software's security vulnerabilities, threats, and weaknesses. The test team may also conduct testing to ensure that the software can protect against unauthorized access, use, disclosure, disruption, modification, or destruction of information.
              <br /><br />
              Security testing is important because it allows developers to identify and address any vulnerabilities, threats, and weaknesses in the software's security before it is released to users. It also helps developers to ensure that the software is designed in a way that provides a high level of security and protection against unauthorized access, use, disclosure, disruption, modification, or destruction of information, which can protect the software's users and the data they store or exchange.
              <br /><br />
              Overall, security testing is an important step in the software development process as it helps developers to evaluate the software's security, identify and address any vulnerabilities, threats, and weaknesses, and ensure that the software is designed in a way that provides a high level of security and protection against unauthorized access, use, disclosure, disruption, modification, or destruction of information. This can protect the software's users and the data they store or exchange, and increase the software's overall value and trust.</p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>2.11.6. Localization & Internationalization</summary>
            <br />
            <p>Localization and internationalization testing are types of non-functional testing that are used to evaluate the software's ability to adapt to different languages, cultures, and regions. These types of testing are focused on ensuring that the software is easily understood and usable by users in different languages, cultures, and regions.
              <br /><br />
              During localization testing, the software is tested in different languages to identify any issues or problems that may arise when the software is translated into different languages. The test team may also conduct testing to ensure that the software is easily understood and usable by users in different languages.
              <br /><br />
              Internationalization testing is the process of testing the software's ability to adapt to different cultures and regions. The test team will conduct testing to ensure that the software is easily understood and usable by users in different cultures and regions. This includes testing for cultural sensitivity, currency, and date/time formats, and making sure that the software adheres to the laws and regulations of different regions.
              <br /><br />
              Localization and internationalization testing are important because they allow developers to identify and address any issues or problems that may arise when the software is translated into different languages or used in different cultures and regions. It also helps developers to ensure that the software is easily understood and usable by users in different languages, cultures, and regions, which can increase the software's overall value and usability.
              <br /><br />
              Overall, localization and internationalization testing are an important step in the software development process as it helps developers to evaluate the software's ability to adapt to different languages, cultures, and regions, identify and address any issues or problems that may arise, and ensure that the software is easily understood and usable by users in different languages, cultures, and regions. This can increase the software's overall value and usability and make it more accessible to a wider range of users.</p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>2.12. Concept of Smoke testing and Sanity Testing</summary>
            <br />
            <p>Smoke testing and sanity testing are both types of functional testing that are used to evaluate the software's basic functionality and overall stability. These types of testing are focused on ensuring that the software is in a stable and working condition before more in-depth testing is conducted.
              <br /><br />
              Smoke testing, also known as "Build Verification Testing" (BVT), is a type of testing that is conducted to ensure that the software's basic functionality is working as expected. It is typically done as soon as a new build of the software is available and is intended to identify any major issues that would prevent the software from being used. This testing is done to make sure that the build is stable enough to proceed with further testing.
              <br /><br />
              Sanity testing, also known as "Health Check Testing" or "Quick Testing" is a type of testing that is conducted to ensure that the software's basic functionality is working as expected. It is typically done after changes have been made to the software and is intended to identify any major issues that would prevent the software from being used. This testing is done to make sure that the software is stable enough to proceed with further testing.
              <br /><br />
              Both Smoke testing and Sanity testing are considered as a minimal level of testing that is done to verify that the software is stable and working correctly. These tests are intended to identify any major issues that would prevent the software from being used and to ensure that the software is stable enough to proceed with further testing.
              <br /><br />
              Overall, smoke testing and sanity testing are an important step in the software development process as it helps developers to evaluate the software's basic functionality and overall stability, identify and address any major issues that would prevent the software from being used, and ensure that the software is stable enough to proceed with further testing. This can increase the software's overall value and reliability and make it more accessible to a wider range of users.</p>
            <br /><br /><br />
          </details>
          <br />
        </details>
      </div>
      <br />
      <div class="unitNo3">
        <details>
          <summary>3  3. Static Testing </summary>
          <br>
          <details>
            <br />
            <summary>3  3. Static Testing </summary>
            <br />
            <p>Static testing is a type of software testing that is focused on evaluating the software's code, design, and documentation without executing the software. This type of testing is done to identify any issues or defects in the software before it is executed or deployed.
              <br /><br />
              Static testing can be done manually or using automated tools. Some common methods of static testing include:
              <br /><br />
              Code reviews: A process in which one or more developers review the code to identify any issues or defects.<br /><br />
              Inspection: A process in which one or more developers review the design and documentation to identify any issues or defects.<br /><br />
              Linting: A process in which automated tools are used to analyze the code to identify any issues or defects.<br /><br />
              Code analysis: A process in which automated tools are used to analyze the code to identify any issues or defects.<br /><br />
              Static testing is an important step in the software development process as it helps developers to identify and address any issues or defects in the software before it is executed or deployed. This can increase the software's overall quality and reliability, and can help to reduce the number of bugs that are discovered during later stages of testing.
              <br /><br />
              Overall, static testing is an important step in the software development process as it helps developers to identify and address any issues or defects in the software before it is executed or deployed. This can increase the software's overall quality and reliability and make it more accessible to a wider range of users.</p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>3.1. Static Techniques ‚Äì Review </summary>
            <br />
            <p>Review is a technique of static testing that involves having one or more developers review the software's code, design, and documentation to identify any issues or defects. This technique can be done manually or with the help of automated tools.
              <br /><br />
              There are different types of reviews that can be used, depending on the goals of the review and the complexity of the software. Some common types of reviews include:
              <br /><br />
              Peer review: A process in which developers review the code written by other developers in their team.<br /><br />
              Technical review: A process in which developers review the software's design and documentation to identify any technical issues or defects.<br /><br />
              Inspection: A formal review process in which developers review the software's design and documentation to identify any issues or defects.<br /><br />
              Walkthrough: A process in which developers review the software's design and documentation to identify any issues or defects.<br /><br />
              The goal of a review is to identify any issues or defects in the software before it is executed or deployed. This can help to increase the software's overall quality and reliability and can help to reduce the number of bugs that are discovered during later stages of testing.
              <br /><br />
              Overall, review is a technique of static testing that involves having one or more developers review the software's code, design, and documentation to identify any issues or defects. This technique can be done manually or with the help of automated tools and its goal is to increase the software's overall quality and reliability.</p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>3.1.1. Review Process (Informal & Formal) </summary>
            <br />
            <p>There are two main types of review process: informal and formal.
              <br /><br />
              Informal review process:
              An informal review process is a less structured and less formal process of reviewing the software's code, design, and documentation. This process is usually done by developers within the same team and is often done on an ad-hoc basis. Informal reviews can be done through a variety of methods, such as peer reviews, code walkthroughs, or pair programming. The goal of an informal review is to catch any obvious issues or defects before the software is executed or deployed.
              <br /><br />
              Formal review process:
              A formal review process is a more structured and formal process of reviewing the software's code, design, and documentation. This process is usually done by a dedicated team of reviewers, who may or may not be developers. Formal reviews are often done using a specific method, such as inspections or walkthroughs. The goal of a formal review is to catch any issues or defects that might have been missed during an informal review and to ensure that the software meets the desired quality standards.
              <br /><br />
              Overall, the review process is a way to check for the software's quality and reliability and to catch any issues or defects before the software is executed or deployed. The choice between an informal or formal review process depends on the software's complexity and the specific goals of the review.</p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>3.1.2. Desk Checking,</summary>
            <br />
            <p>Desk checking is a technique of static testing that involves manually reviewing the software's code, design, and documentation to identify any issues or defects. This technique is also known as "mental testing" or "dry run testing" as it is performed without executing the code.
              <br /><br />
              The goal of desk checking is to identify any errors or defects in the software's logic, design, or documentation before it is executed or deployed. This can help to increase the software's overall quality and reliability and can help to reduce the number of bugs that are discovered during later stages of testing.
              <br /><br />
              The process of desk checking typically involves the following steps:
              <br /><br />
              Reviewing the software's code, design, and documentation.<br /><br />
              Identifying any issues or defects in the software's logic, design, or documentation.<br /><br />
              Documenting any issues or defects that are identified.<br /><br />
              Communicating any issues or defects to the development team for correction.<br /><br />
              Desk checking can be done by developers, quality assurance specialists, or other team members who have a good understanding of the software's requirements and design. It's a technique that can be done manually or with the help of automated tools.
              <br /><br />
              Overall, desk checking is a technique of static testing that involves manually reviewing the software's code, design, and documentation to identify any issues or defects. This technique is performed without executing the code and its goal is to increase the software's overall quality and reliability.
              
              
              
              </p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>3.1.3. Technical or Peer Review</summary>
            <br />
            <p>Technical or Peer Review is a technique of static testing that involves reviewing the software's code, design, and documentation by a team of peers or technical experts. This technique is also known as "code review" or "peer code review" and it is a form of informal review process.
              <br /><br />
              The goal of a technical or peer review is to identify any issues or defects in the software's logic, design, or documentation before it is executed or deployed. This can help to increase the software's overall quality and reliability and can help to reduce the number of bugs that are discovered during later stages of testing.
              <br /><br />
              The process of a technical or peer review typically involves the following steps:
              <br /><br />
              Identifying the software's code, design, and documentation that needs to be reviewed.<br /><br />
              Assigning a team of peers or technical experts to review the software.<br /><br />
              Reviewing the software's code, design, and documentation by the assigned team.<br /><br />
              Identifying any issues or defects in the software's logic, design, or documentation.<br /><br />
              Documenting any issues or defects that are identified.<br /><br />
              Communicating any issues or defects to the development team for correction.<br /><br />
              Technical or Peer Review can be done by developers, quality assurance specialists, or other team members who have a good understanding of the software's requirements and design. It's a technique that can be done manually or with the help of automated tools.</p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>3.1.4. Walkthrough</summary>
            <br />
            <p>Walkthrough is a technique of static testing that involves reviewing the software's code, design, and documentation in a formal meeting with the development team and other stakeholders. This technique is also known as "inspection" and it is a form of formal review process.
              <br /><br />
              The goal of a walkthrough is to identify any issues or defects in the software's logic, design, or documentation before it is executed or deployed. This can help to increase the software's overall quality and reliability and can help to reduce the number of bugs that are discovered during later stages of testing.
              <br /><br />
              The process of a walkthrough typically involves the following steps:
              <br /><br />
              Identifying the software's code, design, and documentation that needs to be reviewed.<br /><br />
              Scheduling a formal meeting with the development team and other stakeholders.<br /><br />
              Reviewing the software's code, design, and documentation in the meeting.
              Identifying any issues or defects in the software's logic, design, or documentation.<br /><br />
              Documenting any issues or defects that are identified.<br /><br />
              Communicating any issues or defects to the development team for correction.
              Walkthrough can be done by developers, quality assurance specialists, or other team members who have a good understanding of the software's requirements and design. It's a technique that can be done manually or with the help of automated tools.
              <br /><br />
              Overall, Walkthrough is a technique of static testing that involves reviewing the software's code, design, and documentation in a formal meeting with the development team and other stakeholders. This technique is a form of formal review process and its goal is to increase the software's overall quality and reliability.</p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>3.1.5. Inspection </summary>
            <br />
            <p>Inspection is a technique of static testing that is similar to walkthrough and involves reviewing the software's code, design, and documentation in a formal meeting with the development team and other stakeholders. It is a rigorous and systematic process that is led by a trained and experienced moderator, known as the "inspector" or "leader", who facilitates the inspection process and ensures that it is conducted in accordance with a defined set of guidelines and procedures.
              <br /><br />
              The goal of an inspection is to identify any issues or defects in the software's logic, design, or documentation before it is executed or deployed. This can help to increase the software's overall quality and reliability and can help to reduce the number of bugs that are discovered during later stages of testing.
              <br /><br />
              The process of an inspection typically involves the following steps:
              <br /><br />
              Identifying the software's code, design, and documentation that needs to be reviewed.<br /><br />
              Scheduling a formal meeting with the development team and other stakeholders.
              Reviewing the soft<br /><br />ware's code, design, and documentation in the meeting.
              Identifying any issues or defects in the software's logic, design, or documentation.<br /><br />
              Documenting any issues or defects that are identified.<br /><br />
              Communicating any issues or defects to the development team for correction.<br /><br />
              Inspection can be done by developers, quality assurance specialists, or other team members who have a good understanding of the software's requirements and design. It's a technique that can be done manually or with the help of automated tools.
              <br /><br />
              Overall, Inspection is a technique of static testing that is similar to walkthrough and involves reviewing the software's code, design, and documentation in a formal meeting with the development team and other stakeholders. It is a rigorous and systematic process that is led by a trained and experienced moderator, known as the "inspector" or "leader", who facilitates the inspection process and ensures that it is conducted in accordance with a defined set of guidelines and procedures. Its goal is to increase the software's overall quality and reliability.
              
              
              
              </p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>3.2. Static Techniques ‚Äì Static Analysis</summary>
            <br />
            <p>Static Analysis is a technique of static testing that involves using automated tools to analyze the software's code, design, and documentation without executing it. The goal of static analysis is to identify any issues or defects in the software's logic, design, or documentation before it is executed or deployed. This can help to increase the software's overall quality and reliability and can help to reduce the number of bugs that are discovered during later stages of testing.
              <br /><br />
              Static analysis tools can be used to perform a wide range of tasks, including:
              <br /><br />
              Identifying coding errors and inconsistencies: These tools can scan the software's code for potential errors, such as syntax errors, semantic errors, and other coding issues.
              <br /><br />
              Detecting security vulnerabilities: These tools can scan the software's code for potential security vulnerabilities, such as SQL injection, cross-site scripting, and other common web application security threats.
              <br /><br />
              Checking compliance with coding standards: These tools can check the software's code for compliance with industry-standard coding conventions, such as best practices for naming conventions and variable declarations.
              <br /><br />
              Identifying potential performance issues: These tools can scan the software's code for potential performance bottlenecks, such as memory leaks or slow-running loops.
              <br /><br />
              Detecting duplicated code: These tools can scan the software's code for duplicated code, which can lead to increased maintenance costs and increased risk of bugs.
              <br /><br />
              Static Analysis can be done by developers, quality assurance specialists, or other team members who have a good understanding of the software's requirements and design. It's a technique that can be done manually or with the help of automated tools.
              <br /><br />
              Overall, Static Analysis is a technique of static testing that involves using automated tools to analyze the software's code, design, and documentation without executing it. The goal of static analysis is to identify any issues or defects in the software's logic, design, or documentation before it is executed or deployed. This can help to increase the software's overall quality and reliability and can help to reduce the number of bugs that are discovered during later stages of testing.
              
              
              
              </p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>3.2.1. Data flow analysis </summary>
            <br />
            <p>Data flow analysis is a technique of static analysis that involves analyzing the flow of data through a software system. The goal of data flow analysis is to understand how data is processed, stored, and used within the system, and to identify any potential issues or defects in the software's logic or design.
              <br /><br />
              Data flow analysis can be used to perform a wide range of tasks, including:
              <br /><br />
              Identifying data dependencies: These tools can identify the relationships between different variables and data structures, and can help to identify any potential issues with data flow or data usage.
              <br /><br />
              Detecting data races: These tools can identify any potential data races, which occur when multiple threads or processes access the same data simultaneously, potentially leading to data corruption or other issues.
              <br /><br />
              Identifying potential performance issues: These tools can analyze the flow of data through the system and identify any potential bottlenecks or performance issues.
              <br /><br />
              Detecting data leaks: These tools can identify any potential data leaks, which occur when sensitive or confidential data is inadvertently shared or exposed.
              <br /><br />
              Identifying potential security vulnerabilities: These tools can identify any potential security vulnerabilities, such as SQL injection or cross-site scripting, that may be caused by poor data flow or data usage.
              <br /><br />
              Data flow analysis can be done by developers, quality assurance specialists, or other team members who have a good understanding of the software's requirements and design. It's a technique that can be done manually or with the help of automated tools.
              <br /><br />
              Overall, Data flow analysis is a technique of static analysis that involves analyzing the flow of data through a software system. The goal of data flow analysis is to understand how data is processed, stored, and used within the system, and to identify any potential issues or defects in the software's logic or design. This can help to increase the software's overall quality and reliability and can help to reduce the number of bugs that are discovered during later stages of testing.</p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>3.2.2. Control flow analysis,</summary>
            <br />
            <p>Control flow analysis is a type of static analysis that focuses on the flow of control through a software system. The goal of control flow analysis is to identify potential issues with the code, such as bugs, vulnerabilities, and other problems, that may arise from the way the code is structured and how it controls the flow of execution.
              <br /><br />
              Control flow analysis is typically performed using specialized software tools that can automatically analyze the code and generate reports on potential issues. These tools can be used to identify a wide range of issues, including:
              <br /><br />
              Unreachable code: This occurs when code is present in the system that can never be executed.
              <br /><br />
              Dead code: This occurs when code is present in the system that is never executed in any possible scenario.
              <br /><br />
              Infinite loops: This occurs when a loop in the code never exits, causing the system to hang or crash.
              <br /><br />
              Logic errors: This occurs when the code is structured in such a way that it produces unexpected or incorrect results.
              <br /><br />
              Exceptions and error handling: This occurs when the code does not properly handle exceptions or errors that may occur during execution.
              <br /><br />
              Control flow analysis can be used to identify potential issues early on in the development process, before they become more costly and difficult to fix. It can also be used to track the progress of the project and identify areas of the code that may need further testing or review.
              <br /><br />
              Overall, Control flow analysis is a type of static analysis that focuses on the flow of control through a software system. The goal of control flow analysis is to identify potential issues with the code such as bugs, vulnerabilities, and other problems that may arise from the way the code is structured and how it controls the flow of execution. It can be an effective way to improve software quality and reduce the number of bugs that are discovered during later stages of testing.
              
              
              
              </p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>3.2.3. Static Analysis by Tools (Automated Static 
              Analysis)  </summary>
            <br />
            <p>Static Analysis by Tools, also known as Automated Static Analysis, is the process of using specialized software tools to perform static analysis on the source code of a software system. The goal of these tools is to automate many of the manual tasks associated with static analysis, such as reviewing code, identifying potential defects and vulnerabilities, and generating reports.
              <br /><br />
              There are a wide range of static analysis tools available, each with its own strengths and capabilities. Some of the most common types of static analysis tools include:
              <br /><br />
              Linting tools: These tools are designed to identify potential issues with the code, such as syntax errors, coding style violations, and other problems.
              <br /><br />
              Code review tools: These tools are designed to perform automated code reviews, identifying potential issues with the code, such as bugs, vulnerabilities, and other problems.
              <br /><br />
              Security analysis tools: These tools are designed to identify potential security vulnerabilities in the code, such as SQL injection, cross-site scripting, and other issues.
              <br /><br />
              Performance analysis tools: These tools are designed to identify potential performance issues in the code, such as bottlenecks, memory leaks, and other problems.
              <br /><br />
              Metrics tools: These tools are designed to gather metrics on the code, such as code complexity, maintainability, and other factors.
              <br /><br />
              These tools can be used by developers, quality assurance specialists, or other team members who have a good understanding of the software's requirements and design. They can be integrated into the development process to detect issues early on, and can also be used to generate reports that can be used to track the progress of the project.
              <br /><br />
              Overall, Static Analysis by Tools (Automated Static Analysis) is the process of using specialized software tools to perform static analysis on the source code of a software system. The goal of these tools is to automate many of the manual tasks associated with static analysis and help to improve the software's overall quality, reliability and security. It also helps to reduce the number of bugs that are discovered during later stages of testing and make the process more efficient and cost-effective.</p>
            <br /><br /><br />
          </details>
          <br />
        </details>
      </div>

      <br />
      <div class="unitNo4">
        <details>
          <summary> 4  4. Dynamic Testing  </summary>
          <br>
          <details>
            <br />
            <summary> 4  4. Dynamic Testing  </summary>
            <br />
            <p>Dynamic testing is a type of software testing that involves executing the software and observing its behavior during runtime. It is also known as behavioral testing or functional testing. The goal of dynamic testing is to identify any issues with the software's functionality, performance, and usability.
              <br /><br />
              Dynamic testing can be performed in a variety of ways, including manual testing and automated testing.
              <br /><br />
              Manual testing: This involves manually executing the software and observing its behavior to identify any issues. This can include testing the software's functionality, performance, and usability.
              <br /><br />
              Automated testing: This involves using specialized software tools to automatically execute the software and observe its behavior. This can include functional testing, load testing, and performance testing.
              <br /><br />
              Dynamic testing can be used to identify a wide range of issues, including:
              <br /><br />
              Functional issues: This occurs when the software does not perform as expected or does not meet the requirements.
              <br /><br />
              Performance issues: This occurs when the software does not perform well under certain conditions, such as when it is under heavy load or when it is used with large amounts of data.
              <br /><br />
              Usability issues: This occurs when the software is difficult to use or when it does not meet the needs of the users.
              <br /><br />
              Dynamic testing can be an effective way to identify issues with the software that may not be discovered during static testing. It can also be used to track the progress of the project and identify areas of the code that may need further testing or review.
              <br /><br />
              Overall, Dynamic Testing is a type of software testing that involves executing the software and observing its behavior during runtime. The goal of dynamic testing is to identify any issues with the software's functionality, performance, and usability. It can be performed in a variety of ways, including manual testing and automated testing, and it can be an effective way to identify issues with the software that may not be discovered during static testing.
              
              
              
              </p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>4.1. Test Design Techniques-Black Box Testing 
              Techniques:  </summary>
            <br />
            <p>Equivalence Partitioning: This technique divides the input data into different partitions, called equivalence classes, and tests each class separately. This helps to identify any errors or defects in the software.
              <br /><br />
              Boundary Value Analysis: This technique focuses on testing the input data at the boundaries of the equivalence classes. This helps to identify any errors or defects that occur at the edges of the input data.
              <br /><br />
              Error Guessing: This technique involves using experience and knowledge of the software to identify potential errors or defects. It is often used in combination with other testing techniques.
              <br /><br />
              Decision Table Testing: This technique involves creating a table that outlines all possible combinations of inputs and outputs for a given feature or function. This helps to identify any errors or defects in the software.
              <br /><br />
              State Transition Testing: This technique involves identifying all possible states of the software and testing the transitions between them. This helps to identify any errors or defects in the software.
              <br /><br />
              Use Case Testing: This technique involves testing the software using realistic scenarios, called use cases, that reflect how the software will be used in the real world. This helps to identify any errors or defects in the software.
              <br /><br />
              Exploratory Testing: This technique involves testing the software in an exploratory manner, without a preconceived plan. This helps to identify any errors or defects in the software that may not have been anticipated.
              <br /><br />
              User Acceptance Testing (UAT): This is a type of Black Box testing where the end users or business stakeholders test the software to determine if it meets their needs and requirements.
              
              
              
              </p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>4.1.1. Equivalence Partitioning </summary>
            <br />
            <p>Equivalence partitioning is a black box testing technique that is used to divide the input data of a software system into different partitions or classes, called equivalence classes. The goal of this technique is to identify any errors or defects in the software by testing each class separately.
              <br /><br />
              For example, if a software system accepts a user's age as input, the equivalence classes could be:
              <br /><br />
              Age is a positive integer (valid input)<br /><br />
              Age is a negative integer or a non-integer (invalid input)<br /><br />
              Age is greater than or equal to 100 (invalid input)<br /><br />
              By testing each class separately, the tester can identify any errors or defects that may occur when the software processes data within that class. This helps to ensure that the software is functioning correctly and handling different types of input data appropriately.
              <br /><br />
              Equivalence partitioning is a simple and effective technique that can be used early in the software development process. It helps to identify any errors or defects in the software early on, which can save time and money in the long run.</p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>4.1.2. Boundary Value Analysis </summary>
            <br />
            <p>Boundary value analysis is a black box testing technique that is used to identify any errors or defects in the software by testing the input data at the boundaries of the equivalence classes. It is used in combination with equivalence partitioning, which divides the input data into different classes or partitions.
              <br /><br />
              For example, if a software system accepts a user's age as input, the equivalence classes could be:
              <br /><br />
              Age is a positive integer between 1 and 100 (valid input)<br /><br />
              Age is less than 1 or greater than 100 (invalid input)<br /><br />
              The boundary values for this example would be 1 and 100. By testing the software with input data at these boundaries, the tester can identify any errors or defects that may occur when the software processes data at the edges of the input data.
              <br /><br />
              Boundary value analysis is a useful technique because it helps to identify errors or defects that may not be discovered through other testing methods. It is particularly useful for testing software systems that have a large number of inputs or a complex input domain.
              <br /><br />
              In summary, boundary value analysis is a technique that focuses on testing the input data at the boundaries of the equivalence classes in order to identify any errors or defects that may occur at the edges of the input data. By testing the software at the edges of the input domain, the tester can identify any errors or defects that may not be discovered through other testing methods.</p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>4.1.3. Decision Table Testing </summary>
            <br />
            <p>Decision table testing is a black box testing technique that is used to test software systems that have multiple inputs and multiple outputs. It is used to identify any errors or defects in the software by testing different combinations of inputs and outputs.
              <br /><br />
              A decision table is a table that lists all the possible combinations of inputs and outputs for a software system. Each row in the table represents a different combination of inputs and outputs, and the corresponding actions that the software should take for that combination.
              <br /><br />
              For example, a decision table for a software system that accepts user input and generates a response based on that input might look like this:</p>
              <br>
              <br>
              <table style="border: 1px solid black;">
                <tr>
                  <th>Inputs</th>
                  <th>Outputs</th>
                  <th>Actions</th>
                </tr>
                <tr>
                  <td>A</td>
                  <td>1</td>
                  <td>Perform action 1</td>
                </tr>
                <tr>
                  <td>A</td>
                  <td>2</td>
                  <td>Perform action 2</td>
                </tr>
                <tr>
                  <td>B</td>
                  <td>1</td>
                  <td>Perform action 3</td>
                </tr>
                <tr>
                  <td>B</td>
                  <td>2</td>
                  <td>Perform action 4</td>
                </tr>
              </table>
              <br /><br />
              <p>By testing the software with each combination of inputs and outputs listed in the decision table, the tester can identify any errors or defects that may occur when the software processes different combinations of inputs and outputs.
                <br /><br />
                Decision table testing is a useful technique for testing software systems that have complex logic or decision-making processes. It helps to identify any errors or defects in the software that may not be discovered through other testing methods. By testing different combinations of inputs and outputs, the tester can ensure that the software is functioning correctly and handling different types of input data appropriately.</p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>4.1.4. State Transition Testing </summary>
            <br />
            <p>State transition testing is a black box testing technique that is used to test software systems that have different states and transitions between those states. It is used to identify any errors or defects in the software by testing the different states and the transitions between them.
              <br /><br />
              A state transition diagram is a visual representation of the different states and transitions of a software system. Each state is represented by a circle or rectangle, and each transition is represented by an arrow. The arrow shows the flow of the system from one state to another.
              <br /><br />
              For example, a state transition diagram for a software system that controls a vending machine might look like this:
              <br /><br />
              [Insert state transition diagram]
              <br /><br />
              In this diagram, the different states of the vending machine are represented by circles (idle, processing, dispensed) and the transitions between these states are represented by arrows.
              <br /><br />
              State transition testing involves testing the software by putting it into different states and then triggering the transitions between those states. By testing the software in different states and triggering different transitions, the tester can identify any errors or defects that may occur when the software changes states.
              <br /><br />
              State transition testing is a useful technique for testing software systems that have complex logic or decision-making processes. It helps to identify any errors or defects in the software that may not be discovered through other testing methods. By testing different states and transitions, the tester can ensure that the software is functioning correctly and handling different types of input data appropriately.
              
              
              
              </p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>4.2. Test Design Techniques -White Box Testing 
              Techniques (coverage based and fault-based) </summary>
            <br />
            <p>Unit testing, also known as component testing, is a method of testing individual units of code, such as functions or methods, to ensure that they are working correctly. The main goal of unit testing is to identify errors early on in the development process, so that they can be fixed before the software is released to the customer.
              <br /><br />
              Test design techniques for unit testing include white box testing techniques, which are based on the internal structure of the code. There are two types of white box testing techniques:
              <br /><br />
              Coverage-based techniques: Coverage-based techniques focus on ensuring that all the lines of code are executed during testing. This helps to ensure that all the code is being tested and that no errors are being missed. Some examples of coverage-based techniques include statement coverage, branch coverage, and path coverage.
              <br /><br />
              Fault-based techniques: Fault-based techniques focus on identifying and testing specific areas of the code that are more likely to contain errors. This can be done by analyzing the code, looking for common error patterns, and testing those areas more thoroughly. Examples of fault-based techniques include mutation testing, and fault injection.
              <br /><br />
              Both these techniques are used to ensure that all the functionalities of the code is tested and that all the possible bugs are identified and fixed.
              
              
              
              </p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>4.2.1. Statement coverage </summary>
            <br />
            <p>Statement coverage is a white box testing technique that measures the percentage of statements in a program that are executed during testing. The goal of statement coverage is to ensure that all statements in the program are executed at least once, which helps to ensure that all the code is being tested and that no errors are being missed.
              <br /><br />
              To achieve statement coverage, a test suite is created that includes test cases that exercise all the statements in the program. The test suite is then executed and the coverage tool records which statements were executed. The percentage of statements executed is then calculated and reported.
              <br /><br />
              Achieving 100% statement coverage is considered a good practice, but it is not always possible or practical. Achieving 100% statement coverage requires a large number of test cases and can be time-consuming and costly. Therefore, a coverage goal of 80-90% is often considered sufficient.
              <br /><br />
              It's important to note that achieving high statement coverage does not guarantee that the program is free of errors or that it is working correctly. It's just a way to ensure that all the code is being tested and it's a starting point for testing the software.</p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>4.2.2. Branch & Decision coverage </summary>
            <br />
            <p>Branch coverage and decision coverage are other white box testing techniques that are related to statement coverage. Both techniques focus on measuring the percentage of branches and decisions in a program that are executed during testing.
              <br /><br />
              Branch coverage is a measure of the percentage of branches (e.g. if-then-else statements) in a program that are executed. The goal of branch coverage is to ensure that all branches in the program are executed at least once, which helps to ensure that all the code is being tested and that no errors are being missed.
              <br /><br />
              Decision coverage is a measure of the percentage of decisions (e.g. conditions) in a program that are executed. The goal of decision coverage is to ensure that all decisions in the program are executed with all possible outcomes, which helps to ensure that all the code is being tested and that no errors are being missed.
              <br /><br />
              Like statement coverage, achieving 100% branch coverage and decision coverage is considered a good practice, but it's not always possible or practical. Achieving high branch coverage and decision coverage requires a large number of test cases and can be time-consuming and costly. Therefore, a coverage goal of 80-90% is often considered sufficient.
              <br /><br />
              It's important to note that achieving high branch coverage and decision coverage does not guarantee that the program is free of errors or that it is working correctly. It's just a way to ensure that all the code is being tested and it's a starting point for testing the software.</p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>4.2.3. Path coverage</summary>
            <br />
            <p>Path coverage is another white box testing technique that measures the percentage of all possible paths through a program that are executed during testing. The goal of path coverage is to ensure that all possible paths through the program are executed at least once, which helps to ensure that all the code is being tested and that no errors are being missed.
              <br /><br />
              To achieve path coverage, a test suite is created that includes test cases that exercise all possible paths through the program. This can be a very complex and time-consuming task, especially for large and complex programs. The test suite is then executed and the coverage tool records which paths were executed. The percentage of paths executed is then calculated and reported.
              <br /><br />
              Achieving 100% path coverage is considered a good practice, but it is not always possible or practical. Achieving 100% path coverage requires a large number of test cases and can be time-consuming and costly. Therefore, a coverage goal of 80-90% is often considered sufficient.
              <br /><br />
              It's important to note that achieving high path coverage does not guarantee that the program is free of errors or that it is working correctly. It's just a way to ensure that all the code is being tested and it's a starting point for testing the software.
              <br /><br />
              Additionally, Path coverage is harder to achieve than statement and branch coverage, mainly due to the complexity of the software.</p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>4.2.4. McCabe‚Äôs Cyclomatic Complexity Metric 
              (Computation of Cyclomatic Complexity to be 
              covered) </summary>
            <br />
            <p>McCabe's Cyclomatic Complexity Metric, also known as McCabe's complexity, is a software metric that measures the complexity of a program by counting the number of linearly independent paths through the source code. It is used to determine the complexity of a program and to identify areas that are particularly complex and might require additional testing.
              <br /><br />
              The cyclomatic complexity of a program can be computed using the following formula:
              <br /><br />
              CC = E - N + 2P
              <br /><br />
              Where:
              <br /><br />
              E is the number of edges in the control flow graph<br /><br />
              N is the number of nodes in the control flow graph<br /><br />
              P is the number of connected components (program entry and exit points)<br /><br />
              The cyclomatic complexity is a measure of the number of independent paths through the program. A program with a high cyclomatic complexity is considered to be more complex and more difficult to test than a program with a lower cyclomatic complexity.
              <br /><br />
              To compute the cyclomatic complexity of a program, a control flow graph is created, which is a graph that represents the flow of control through the program. The graph is created by breaking down the program into its individual statements and representing each statement as a node. The edges in the graph represent the flow of control between the statements.
              <br /><br />
              Once the control flow graph is created, the number of nodes, edges and connected components is counted, and the cyclomatic complexity is calculated using the formula above.
              <br /><br />
              It's important to note that McCabe's Cyclomatic Complexity Metric is just one of many software metrics that can be used to measure the complexity of a program, and it has some limitations. However, it is widely used in software engineering as it provides a quick and easy-to-understand measure of the structural complexity of a program.
              
              
              
              </p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>4.2.5. Data Flow based Testing </summary>
            <br />
            <p>Data Flow based testing is a white box testing technique that is used to test the functionality of a software application by analyzing the flow of data through the application. This technique is based on the principle that the functionality of a software application is closely related to the way data is processed and transformed as it flows through the system.
              <br /><br />
              The basic idea behind data flow testing is to identify the data inputs and outputs of a software application and the various transformations that are applied to the data as it flows through the system. This can be done by creating a data flow diagram (DFD) which is a graphical representation of the data flows, data stores, and processes in a software application.
              <br /><br />
              Once the DFD is created, test cases can be designed to cover different data flows, data stores, and processes in the application. The test cases are designed to ensure that all possible data flows are tested, and that the data is processed and transformed correctly as it flows through the system.
              <br /><br />
              Data flow testing is a powerful technique that can be used to identify defects in a software application that would be difficult to find using other techniques, such as black box testing. It's particularly useful for complex systems, where it can be difficult to understand the interactions between different components and the flow of data through the system.
              <br /><br />
              It's important to note that data flow testing is usually used in combination with other testing techniques, such as boundary value analysis, equivalence partitioning, and cause-effect graphing, to ensure that a comprehensive set of test cases is created to cover all possible scenarios.</p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>4.2.6. Mutation Testing</summary>
            <br />
            <p>Mutation testing is a technique used to measure the effectiveness of a test suite by introducing small, controlled changes (or "mutations") to the source code and then re-running the test suite to see if it can detect the changes. The idea behind mutation testing is that if a test suite is able to detect the changes, it is likely that it is of high quality and is capable of detecting real defects in the code.
              <br /><br />
              The process of mutation testing involves creating a set of mutants (or "mutated versions") of the original source code by making small, controlled changes, such as changing a comparison operator or negating a Boolean expression. The test suite is then run against each mutant, and if the test suite fails to detect the change, the mutant is considered to have survived. The percentage of mutants that are detected by the test suite is known as the mutation score, and is used to measure the effectiveness of the test suite.
              <br /><br />
              Mutation testing is a powerful technique that can be used to identify defects in a software application that would be difficult to find using other techniques, such as black box testing. It is particularly useful for complex systems, where it can be difficult to understand the interactions between different components and the flow of data through the system.
              <br /><br />
              However, mutation testing is a time-consuming and resource-intensive process, as it requires running the test suite against multiple mutants. Additionally, it can be difficult to create mutants that are representative of real-world defects, and the results of mutation testing can be influenced by the design and implementation of the test suite.
              <br /><br />
              It's important to note that mutation testing is usually used in combination with other testing techniques, such as boundary value analysis, equivalence partitioning, and cause-effect graphing, to ensure that a comprehensive set of test cases is created to cover all possible scenarios.</p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>4.3. Test Design Techniques -Experience based 
              techniques: </summary>
            <br />
            <p>Experience-based testing techniques are methods of creating test cases that are based on the experience and knowledge of the tester. These techniques can be used to supplement other, more formal test design techniques, such as white-box and black-box testing. Some examples of experience-based testing techniques include:
              <br /><br />
              Exploratory testing: This is an informal testing technique where the tester actively explores the software to identify defects and test its functionality. Exploratory testing is often done early in the development process, before the requirements are fully defined or the design is complete.
              <br /><br />
              Error guessing: This technique involves using the tester's knowledge and experience to predict where defects are likely to occur in the software, and designing test cases to detect these defects.
              <br /><br />
              Use case testing: This technique involves testing the software using scenarios that represent how the user is likely to interact with the software in the real-world.
              <br /><br />
              User story testing: This technique involves testing the software using scenarios that are based on user stories, which describe the functionality from the user's perspective.
              <br /><br />
              Risk-based testing: This technique involves identifying the areas of the software that are most likely to contain defects or be the source of problems, and focusing the testing effort on those areas.
              <br /><br />
              Experience-based testing techniques are useful because they can help identify defects that may be missed by more formal test design techniques. However, they are also less structured and less repeatable than formal techniques, which can make it difficult to ensure that all possible defects are found.
              
              
              
              </p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>4.3.1. Error Guessing</summary>
            <br />
            <p>Error guessing is a testing technique where the tester uses their knowledge and experience to predict where defects are likely to occur in the software, and designs test cases to detect these defects. The tester can use their understanding of the software's functionality, the requirements, and the design to identify potential problem areas and create test cases that target those areas.
              <br /><br />
              This technique is often used in combination with other testing techniques, such as white-box testing and black-box testing, to supplement the coverage of the test cases.
              <br /><br />
              One of the main benefits of error guessing is that it can help identify defects that may be missed by other testing techniques. For example, if a requirement is not fully defined, or if the design is incomplete, it may be difficult to use formal test design techniques to create test cases that cover all possible defects. Error guessing can help fill in these gaps by targeting areas that the tester knows are likely to contain defects.
              <br /><br />
              However, error guessing can also have some drawbacks. Since it is based on the tester's knowledge and experience, it is less repeatable and less structured than formal testing techniques. This can make it difficult to ensure that all possible defects are found. Additionally, if the tester's knowledge or experience is limited, they may not be able to identify all of the problem areas in the software.
              
              
              
              </p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>4.3.2. Exploratory Testing</summary>
            <br />
            <p>Exploratory testing is an informal testing technique where the tester actively explores the software while designing and executing test cases. The tester can use their understanding of the software's functionality, the requirements, and the design to identify potential problem areas and create test cases that target those areas. The goal of exploratory testing is to identify defects as quickly as possible, by using a combination of test design, execution and learning.
              <br /><br />
              Exploratory testing is often used to discover new defects or to validate that the software is working as expected. It is a flexible and creative approach to testing, where the tester can take advantage of new information and insights as they arise.
              <br /><br />
              Some of the benefits of exploratory testing are its flexibility, it allows testers to find defects that other testing techniques may miss, and it can help testers learn more about the software and its behavior.
              <br /><br />
              On the other hand, Exploratory testing can be less structured and less repeatable than other testing techniques, and it can be difficult to ensure that all possible defects are found. Additionally, it can be more difficult to measure the effectiveness of exploratory testing, since it relies on the tester's knowledge and experience</p>
            <br /><br /><br />
          </details>
          <br />
        </details>
      </div>
      <br />
      <div class="unitNo5">
        <details>
          <summary>5  5. Test Management </summary><br />
          
          <details>  
            <br>
            <summary>5  5. Test Management </summary>
            <br />
            <p>Test management is the process of planning, organizing, and controlling the testing activities throughout the software development life cycle (SDLC). It involves managing the resources, schedules, and deliverables required to ensure that the software is of high quality and meets the customer's requirements.
              <br /><br />
              Some of the key activities involved in test management include:
              <br /><br />
              Test planning: This involves creating a test strategy, identifying the test objectives, and determining the resources and schedules required for the testing process.
              <br /><br />
              Test design: This involves creating test cases, test scripts, and test data to ensure that the software is tested thoroughly and effectively.
              <br /><br />
              Test execution: This involves running the test cases and scripts to identify defects in the software.
              <br /><br />
              Test monitoring and control: This involves tracking the progress of the testing process and making any necessary adjustments to ensure that the testing is completed on schedule and within budget.
              <br /><br />
              Test reporting: This involves documenting the results of the testing process and communicating the findings to the relevant stakeholders.
              <br /><br />
              Test management is an important part of the software development process, as it helps ensure that the software is of high quality and meets the customer's requirements. It also helps to identify and resolve any issues that arise during the testing process, and it helps to ensure that the software is delivered on time and within budget.
              
              
              
              </p>
            <br /><br /><br />
          </details>
          <br />
          <details>  
            <br>
            <summary>5.1. Test Organization- Roles & Skills of Tester, Test 
              Lead, Test Manager </summary>
            <br />
            <p>Test organization is the structure and management of the testing team and their responsibilities. The roles and skills of testers, test leads, and test managers are key elements of test organization.
              <br /><br />
              Testers: Testers are responsible for executing test cases, identifying defects in the software, and reporting the results of the testing process. They should have strong analytical skills, attention to detail, and be able to work well in a team.
              <br /><br />
              Test Lead: Test leads are responsible for managing the testing team, coordinating the testing activities, and communicating with stakeholders. They should have strong leadership and project management skills, as well as a good understanding of the testing process.
              <br /><br />
              Test Manager: Test managers are responsible for the overall test organization, including test planning, test design, test execution, test monitoring and control, and test reporting. They should have a strong understanding of the software development process, as well as excellent leadership and management skills.
              <br /><br />
              In addition to these roles, there might be other roles like Test Analyst, Test Engineer, Test Consultant etc depending on the organization and project requirements. These roles may have different responsibilities and skills, but they all work together to ensure that the software is of high quality and meets the customer's requirements.</p>
            <br /><br /><br />
          </details>
          <br />
          <details>  
            <br>
            <summary>5.2. Test Planning- Test Plan as per IEEE 829 STANDARD 
              TEST PLAN TEMPLATE </summary>
            <br />
            <p>Test planning is the process of identifying the testing objectives, creating a test strategy, and creating a detailed test plan. The IEEE 829 standard provides a template for test plan documentation that outlines the key elements that should be included in a test plan.
              <br /><br />
              The IEEE 829 standard test plan template includes the following sections:
              <br /><br />
              Introduction: This section provides an overview of the test plan, including the purpose, scope, and references.
              <br /><br />
              Test Items: This section lists the software and hardware items that will be tested.
              <br /><br />
              Features to be tested: This section lists the specific features or functions that will be tested.
              <br /><br />
              Features not to be tested: This section lists the features or functions that will not be tested.
              <br /><br />
              Approach: This section describes the overall testing approach, including the testing methodologies and tools that will be used.
              <br /><br />
              Item Pass/Fail Criteria: This section defines the criteria that must be met for a test item to be considered a pass or a fail.
              <br /><br />
              Suspension Criteria and Resumption Requirements: This section defines the conditions under which testing will be suspended and resumed.
              <br /><br />
              Test Deliverables: This section lists the test deliverables, such as test cases, test scripts, and test reports.
              <br /><br />
              Testing Tasks: This section outlines the specific testing tasks that will be performed, including the test design, test execution, and test reporting.
              <br /><br />
              Environmental Needs: This section lists the hardware, software, and other resources that are needed for testing.
              <br /><br />
              Responsibilities: This section outlines the roles and responsibilities of the testing team.
              <br /><br />
              Schedule: This section provides a schedule for the testing activities.
              <br /><br />
              Budget: This section provides an estimate of the costs associated with testing.
              <br /><br />
              Risks and contingencies: This section identifies potential risks and contingencies, along with the associated mitigation plans.
              <br /><br />
              Approvals: This section lists the individuals or groups who have reviewed and approved the test plan.
              <br /><br />
              It's important to note that the IEEE 829 standard test plan template is a widely used standard, but the exact content and format of a test plan can vary depending on the organization or project. The test plan should be tailored to meet the specific needs of the project and should be reviewed and updated as necessary throughout the testing process.</p>
            <br /><br /><br />
          </details>
          <br />
          <details>  
            <br>
            <summary>5.3. Test Process Monitoring & Control</summary>
            <br />
            <p>Test process monitoring and control involves monitoring the progress of the testing process, comparing it to the plan, and taking corrective action as necessary. This includes:
              <br /><br />
              Tracking and reporting on test progress<br /><br />
              Identifying and managing risks<br /><br />
              Managing and reporting on defects<br /><br />
              Managing and reporting on test metrics<br /><br />
              Managing and reporting on test status<br /><br />
              Managing and reporting on test deliverables<br /><br />
              The goal of test process monitoring and control is to ensure that the testing process is running smoothly and to make sure that any issues are identified and resolved as quickly as possible. This helps to ensure that the final product is of high quality and meets the needs of the users.</p>
            <br /><br /><br />
          </details>
          <br />
          <details>  
            <br>
            <summary>5.3.1. Test Monitoring through -Test Log (IEEE 829: 
              TEST LOG TEMPLATE to be discussed) and 
              Defect Density </summary>
            <br />
            <p>Test monitoring through test logs and defect density is a way to keep track of the testing process and identify areas where improvements can be made.
              <br /><br />
              A test log is a record of all the testing activities that have taken place. It includes information such as test cases executed, defects found, and test results. The IEEE 829 standard for test logs includes a template that specifies the information that should be included in a test log. The template includes sections for test case identification, test procedure identification, test inputs, test results, and a pass/fail indicator.
              <br /><br />
              Defect density is a measure of the number of defects found in a specific area of the software. It is calculated by dividing the number of defects found by the size of the code or the number of lines of code. Defect density can be used to identify areas of the software that have a higher number of defects, which can help to focus testing efforts on those areas.
              <br /><br />
              Test monitoring through test logs and defect density helps to identify areas of the software that need more testing and areas where improvements can be made in the development and testing process. This helps to ensure that the final product is of high quality and meets the needs of the users.</p>
            <br /><br /><br />
          </details>
          <br />
          <details>  
            <br>
            <summary>5.3.2. Reporting Test Status (IEEE 829: TEST 
              SUMMARY REPORT TEMPLATE to be discussed)</summary>
            <br />
            <p>Reporting test status is an important aspect of test management as it helps to communicate the progress and results of the testing process to stakeholders.
              <br /><br />
              The IEEE 829 standard for software testing includes a template for a test summary report. The template includes sections for identifying the project and the report, providing an overview of the testing process, and summarizing the results of the testing. The template also includes sections for identifying the number of test cases executed, the number of defects found, the number of defects fixed, and the number of defects remaining.
              <br /><br />
              The test summary report should be used to communicate the overall status of the testing process and the results of the testing. It should provide an overview of the testing activities, the results of the testing, and any issues that were encountered. The report should be clear and easy to understand, and it should be presented in a format that is easy to read and navigate.
              <br /><br />
              The test summary report should be provided to the project manager, the development team, and any other stakeholders who need to be informed about the progress and results of the testing process. It can also be used as an input to the project plan for the next phase of the project.
              
              
              
              </p>
            <br /><br /><br />
          </details>
          <br />
          <details>  
            <br>
            <summary>5.3.3. Test Control </summary>
            <br />
            <p>Test control is the process of managing and overseeing the testing process to ensure that it is executed effectively and efficiently. It involves monitoring the progress of the testing, identifying and addressing any issues that arise, and making any necessary adjustments to the testing plan to ensure that the testing is completed on time and within budget.
              <br /><br />
Test control can be divided into two main activities: monitoring and controlling.
<br /><br />
Monitoring involves keeping track of the testing process to ensure that it is on schedule and that the test cases are being executed as planned. It includes tracking the number of test cases that have been executed, the number of defects found, and the number of defects that have been fixed.
<br /><br />
Controlling involves taking action to address any issues that arise during the testing process. This may include revising the testing plan, adjusting the test schedule, or allocating additional resources to the testing process. It also includes monitoring and controlling the overall progress of testing, and taking corrective action when needed.
<br /><br />
The test lead or test manager is responsible for test control. They should have a good understanding of the testing process, the testing tools and techniques, and the testing schedule. They should also be able to work closely with the project manager and the development team to ensure that the testing is completed on time and within budget.
<br /><br />
Overall, test control is important to ensure that the testing process is executed effectively, efficiently, and successfully. It helps to ensure that the software meets the quality standards, and it helps to identify and resolve any issues that arise during the testing process.</p>
            <br /><br /><br />
          </details>
          <br />
          <details>  
            <br>
            <summary>5.4. Requirement Traceability Matrix (Horizontal & 
              Vertical), Test Scenario, Test Suite, Test Cases (both 
              Positive & Negative Test Cases, as per IEEE 829: TEST CASE SPECIFICATION TEMPLATE) </summary>
            <br />
            <p>Test control is the process of managing the execution of tests and tracking the results. It includes activities such as tracking test progress, identifying and resolving issues that arise during testing, and taking corrective action as needed.
              <br /><br />
              A Requirement Traceability Matrix (RTM) is a tool that helps to ensure that all requirements are tested by providing a mapping between requirements and test cases. RTM can be represented both horizontally and vertically.
              <br /><br />
              Test Scenario is a document that describes the overall approach to testing a specific feature or functionality of the software. Test Suite is a collection of test cases that are grouped together to test a specific feature or functionality.
              <br /><br />
              Test cases are the individual tests that are executed to validate the software. They can be divided into positive test cases, which test the system's ability to function correctly, and negative test cases, which test the system's ability to handle incorrect input or other error conditions.
              <br /><br />
              IEEE 829 is a standard that provides guidelines for the preparation of software test documentation, including the Test Plan, Test Log, Test Summary Report, and Test Case Specification templates. These templates are used to ensure consistency and completeness in the test documentation.</p>
            <br /><br /><br />
          </details>
          <br />
          <details>  
            <br>
            <summary>5.5. Configuration Management- Configuration 
              Management support for Testing </summary>
            <br />
            <p>Configuration Management (CM) is the process of identifying, organizing, controlling, and tracking changes made to software and hardware throughout the development and maintenance of a product. In the context of software testing, CM supports the testing process by ensuring that the correct version of the software is being tested, and that any changes made to the software during testing are tracked and managed properly. This can include activities such as version control, change control, and release management. By implementing a robust CM process, organizations can ensure that the software being tested is of the highest quality and that any issues that arise during testing can be quickly identified and resolved.</p>
            <br /><br /><br />
          </details>
          <br />
          <details>  
            <br>
            <summary>5.6. Risk and Testing- Project Risk & Product Risk </summary>
            <br />
            <p>Risk and testing are closely related in software development as testing is done to identify and mitigate risks associated with the software. Project risk refers to the potential negative impact on the project schedule, budget, or performance if an issue or problem occurs during the project. Product risk, on the other hand, refers to the potential negative impact on the product or service being developed if an issue or problem occurs during the development or maintenance of the product.
              <br /><br />
              Testing plays a crucial role in identifying and mitigating both project and product risks. By thoroughly testing the software, testers can identify and report any defects or issues that could potentially lead to project delays or increased costs. Additionally, by testing the software early and often, testers can catch issues before they become critical, reducing the overall product risk.
              <br /><br />
              To manage the risk and testing, organizations can use various testing strategies such as Risk-Based Testing, where the focus is on testing the highest risk areas of the software first. Additionally, testing teams can also use tools like risk management software or a risk matrix to identify and prioritize risks and determine the most effective testing approach to mitigate them.
              
              
              
              </p>
            <br /><br /><br />
          </details>
          <br />
          <details>  
            <br>
            <summary>5.7. Incident/ Defect Management</summary>
            <br />
            <p>Incident/Defect management is the process of tracking, managing, and resolving issues or defects that are found during the testing process. This includes the identification, classification, and tracking of defects, as well as the communication and resolution of these issues with the relevant stakeholders. It also involves the management of a defect database, which is used to store information about the defects, including their status, priority, and resolution. Effective incident/defect management is critical for maintaining the quality of the software and ensuring that defects are resolved in a timely manner, thus reducing the risk of software failure.



            </p>
            <br /><br /><br />
          </details>
          <br />
          <details>  
            <br>
            <summary>5.7.1. Defect Life Cycle</summary>
            <br />
            <p>The Defect Life Cycle, also known as the Bug Life Cycle, is the process that a defect, or bug, goes through from its discovery to its resolution. The exact steps and terminology may vary depending on the organization, but generally, the Defect Life Cycle includes the following steps:
              <br /><br />
              New: The defect is reported and is in its initial state.<br /><br />
              Open: The defect is being investigated to determine if it is a valid issue.<br /><br />
              Assigned: The defect has been determined to be a valid issue and is assigned to a developer for resolution.<br /><br />
              Fixed: The developer has fixed the defect and the fix is being tested.<br /><br />
              Verified: The fix has been tested and is determined to be effective.<br /><br />
              Closed: The defect has been resolved and is no longer an issue.<br /><br />
              It is important to have a clear Defect Life Cycle in place as it allows for efficient tracking and management of defects and helps ensure that all defects are addressed in a timely manner.</p>
            <br /><br /><br />
          </details>
          <br />
          <details>  
            <br>
            <summary>5.7.2. Defect/ Incident Report (IEEE 829: TEST 
              INCIDENT REPORT TEMPLATE to be discussed)</summary>
            <br />
            <p>Defect/ Incident management is the process of tracking and managing the issues or defects found during the testing phase of the software development life cycle. It involves identifying, reporting, and resolving defects in a timely and efficient manner to ensure the quality and functionality of the software product. The Defect Life Cycle is the sequence of steps that a defect goes through, starting from the time it is identified, until it is closed.
              <br /><br />
              The Defect/ Incident Report is a document that is used to record and track defects or incidents that are found during testing. It typically includes information such as the defect/ incident ID, description, priority, and status. The IEEE 829 Test Incident Report template is a widely used template for recording and tracking defects and incidents in software development. It includes information such as the incident ID, description, priority, and status, as well as details on the steps taken to resolve the issue.</p>
            <br /><br /><br />
          </details>
          <br />
        </details>
      </div>
      <br />
      <div class="unitNo6">
        <details>
          <summary>6   6. Tool Support for Testing</summary>
          <br>
          <details>
            <br />
            <summary>6   6. Tool Support for Testing </summary>
            <br />
            <p>There are a variety of tools available to support software testing. Some common categories include:
              <br /><br />
              Test management tools: These tools help to manage and track the testing process, including test cases, test execution, and defects. Examples include HP Quality Center, IBM Rational Quality Manager, and TestRail.
              <br /><br />
              Test automation tools: These tools allow tests to be automated, reducing the time and effort required to run them manually. Examples include Selenium, Appium, and TestComplete.
              <br /><br />
              Performance testing tools: These tools are used to test the performance of software, including load testing and stress testing. Examples include Apache JMeter, LoadRunner, and Gatling.
              <br /><br />
              Code coverage tools: These tools measure the amount of code that is executed during testing, helping to identify areas of the code that are not being tested. Examples include Cobertura, JaCoCo, and Clover.
              <br /><br />
              Static analysis tools: These tools analyze code without executing it, helping to identify potential bugs and security vulnerabilities. Examples include Checkstyle, FindBugs, and SonarQube.
              <br /><br />
              It's important to note that the right tool for your organization will depend on your specific needs and requirements. It's good practice to evaluate different tools and select the one that best fits your organization.</p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>6.1. Types of Test tools CAST (only type & their 
              purpose should be covered) </summary>
            <br />
            <p>CAST (Computer Aided Software Testing) tools are software applications that assist in various aspects of testing, such as test management, test execution, test automation, and test analysis. There are various types of CAST tools available, each serving a specific purpose:
              <br /><br />
              Test Management Tools: These tools help manage and organize the testing process, including test planning, test execution, and test tracking.
              <br /><br />
              Test Automation Tools: These tools automate the execution of test cases and test scripts, reducing the time and effort required for manual testing.
              <br /><br />
              Test Execution Tools: These tools execute the test cases and provide the results, including pass/fail status and any defects or issues that may be encountered.
              <br /><br />
              Test Analysis Tools: These tools analyze the test results and provide metrics and reports on the software's quality and performance.
              <br /><br />
              Performance Testing Tools: These tools are used to test the performance of a software application, including response time, throughput, and scalability.
              <br /><br />
              Security Testing Tools: These tools are used to test the security of a software application, identifying vulnerabilities and potential threats.
              <br /><br />
              Defect Tracking and Management Tools: These tools are used to manage and track defects and issues encountered during the testing process.
              <br /><br />
              Test Data Generation Tools: These tools are used to generate test data for use in testing the software application.</p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>6.2. Effective Use of Tools: Potential Benefits and Risks</summary>
            <br />
            <p>Effective use of tools in software testing can bring many benefits, such as:
              <br /><br />
              Automation of repetitive and time-consuming tasks, allowing testers to focus on more complex and critical testing activities.<br /><br />
              Increased efficiency and speed of testing, resulting in faster delivery of high-quality software.<br /><br />
              Improved accuracy and consistency in testing, reducing the likelihood of human errors.<br /><br />
              Better tracking and reporting of testing progress and results.<br /><br />
              However, it's important to note that there are also risks associated with the use of test tools, such as:<br /><br />
              
              Dependence on the tool, which can lead to a lack of understanding of the underlying testing process.<br /><br />
              Overreliance on the tool's capabilities, which can lead to overlooking important testing activities.<br /><br />
              Difficulty in integrating the tool with other tools and systems in the organization.<br /><br />
              High cost of purchasing and maintaining the tool.<br /><br />
              To mitigate these risks, it is important to select the appropriate tools for the specific testing needs, train personnel on their use, and regularly review and evaluate their effectiveness in meeting the testing objectives.</p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <summary>6.3. Introduction of a tool into an organization </summary>
            <br />
            <p>Introduction of a tool into an organization typically involves the following steps:
              <br /><br />
              Identifying the need for the tool: This step involves identifying the specific problems or areas that the tool is intended to address.<br /><br />
              Evaluating the tool: This step involves researching and evaluating different tools that are available in the market to determine which one best meets the organization's needs.<br /><br />
              Selecting the tool: After evaluating the different tools, the organization selects the one that best meets its needs.<br /><br />
              Implementing the tool: This step involves installing and configuring the tool, as well as training the users on how to use it.<br /><br />
              Maintaining the tool: This step involves monitoring and maintaining the tool to ensure that it continues to meet the organization's needs and is up-to-date with the latest features and security updates.<br /><br />
              Continuously evaluating the tool: After the tool has been implemented, the organization should continuously evaluate the tool's performance and make any necessary adjustments.
              
              
              </p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>6.4. Testing tools</summary>
            <br />
            <p>Testing tools are software applications that are designed to assist in the testing process. They can be used to automate repetitive tasks, perform tests at a higher level of abstraction, and provide metrics and reporting on the testing process. Some examples of testing tools include:
              <br /><br />
Automated testing tools, such as Selenium, Appium, and Cucumber, which are used to automate functional and acceptance testing.<br /><br />
Performance testing tools, such as Apache JMeter and LoadRunner, which are used to test the performance and scalability of an application.<br /><br />
Test management tools, such as TestRail and qTest, which are used to manage and track the progress of testing, as well as defects and incidents.<br /><br />
Test case generation tools, such as JUnit and TestNG, which are used to generate test cases and test suites for automated testing.<br /><br />
It's important to note that the selection of a testing tool should be based on the specific needs of the organization and the testing process. Also, the effective use of tools requires proper training, setting up of environments and proper maintenance.</p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>6.4.1. Selenium -WebDriver and Test NG </summary>
            <br />
            <p>Selenium is an open-source suite of tools for automating web browsers. It is widely used for web application testing and supports many programming languages including Java, C#, Python, and Ruby.
              <br /><br />
              WebDriver, a part of the Selenium Suite, is a tool for automating web browsers. It allows you to control a web browser by programmatically clicking links, filling out forms, and performing other actions.
              <br /><br />
              TestNG is a testing framework for the Java programming language that helps in effective execution of automated unit tests. It is designed to cover all categories of tests: unit, functional, end-to-end, integration, etc. It is an open-source tool that allows you to organize, execute, and report on tests. It is often used in conjunction with Selenium WebDriver to perform automated testing of web applications.</p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>6.4.2. Appium</summary>
            <br />
            <p>Appium is an open-source tool for automating mobile app testing. It is a cross-platform tool that can be used to test apps on iOS and Android platforms. Appium uses the WebDriver protocol to communicate with the app and automates its interactions just like a real user. It supports a wide range of programming languages such as Java, Ruby, Python, C#, and more. Appium allows developers to write tests in a language they are comfortable with, and the same tests can be used for both iOS and Android platforms. It is widely used for functional and performance testing of mobile apps.</p>
            <br /><br /><br />
          </details>
          <br />
          <details>
            <br />
            <summary>6.4.3. JMeter </summary>
            <br />
            <p>JMeter is a Java-based open-source tool used for performance testing, load testing, and functional testing of web applications. It is designed to simulate a heavy load on a server, network, or application and measure the performance of the system under that load. JMeter can be used to test the performance of both static and dynamic resources (files and scripts) and can be used to test many different server types, including HTTP, HTTPS, FTP, SOAP, JDBC, and more. It also has the ability to test the performance of various protocols and technologies such as SOAP, REST, and JMS. Additionally, it can be used to analyze and measure the performance of various services such as databases, LDAP, and JMS. It can also be used to test the performance of a variety of different applications, including web applications, web services, and databases.</p>
            <br /><br /><br />
          </details>
          <br />
        </details>
      </div>
    </section>
    <br /><br /><br /><br />
    <footer class="top-banner">
      <div class="container">
        <div class="small-bold-text banner-text">
          Copyright ¬© 2023 by Atul Nagose (üß† MCA-Gyan üìö)...
        </div>
      </div>
    </footer>
  </body>
</html>
